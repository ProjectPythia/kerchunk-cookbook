{"version":"1","records":[{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)"},"content":"\n\n\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers using the \n\nKerchunk, \n\nVirtualiZarr, and \n\nZarr-Python libraries to access archival data formats as if they were ARCO (Analysis-Ready-Cloud-Optimized) data.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl2":"Motivation"},"content":"The Kerchunk library pioneered the access of chunked and compressed\ndata formats (such as NetCDF3. HDF5, GRIB2, TIFF & FITS), many of\nwhich are the primary data formats for many data archives, as if\nthey were in ARCO formats such as Zarr which allows for parallel,\nchunk-specific access. Instead of creating a new copy of the dataset\nin the Zarr spec/format, Kerchunk reads through the data archive\nand extracts the byte range and compression information of each\nchunk, then writes that information to a “virtual Zarr store” using a\nJSON or Parquet “reference file”. The VirtualiZarr\nlibrary provides a simple way to create these “virtual stores” using familiary\nxarray syntax. Lastly, the icechunk provides a new way to store and re-use these references.\n\nThese virtual Zarr stores can be re-used and read via \n\nZarr and\n\n\nXarray.\n\nFor more details on how this process works please see this page on the\n\n\nKerchunk docs).","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl2":"Authors"},"content":"Raphael Hagen\n\nMuch of the content of this cookbook was inspired by\n\n\nMartin Durant,\nthe creator of Kerchunk and the\n\n\nKerchunk documentation.","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl2":"Structure"},"content":"This cookbook is broken up into two sections,\nFoundations and Example Notebooks.","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Section 1 - Foundations","lvl2":"Structure"},"type":"lvl3","url":"/#section-1-foundations","position":10},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Section 1 - Foundations","lvl2":"Structure"},"content":"In the Foundations section we will demonstrate\nhow to use Kerchunk and VirtualiZarr to create reference files\nfrom single file sources, as well as to create\nmulti-file virtual Zarr stores from collections of files.","type":"content","url":"/#section-1-foundations","position":11},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Section 2 - Generating Virtual Zarr Stores","lvl2":"Structure"},"type":"lvl3","url":"/#section-2-generating-virtual-zarr-stores","position":12},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Section 2 - Generating Virtual Zarr Stores","lvl2":"Structure"},"content":"The notebooks in the Generating Virtual Zarr Stores section\ndemonstrates how to use Kerchunk and VirtualiZarr to create\ndatasets for all the supported file formats.\nThese libraries currently support virtualizing NetCDF3,\nNetCDF4/HDF5, GRIB2, TIFF (including COG).","type":"content","url":"/#section-2-generating-virtual-zarr-stores","position":13},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Section 3 - Using Virtual Zarr Stores","lvl2":"Structure"},"type":"lvl3","url":"/#section-3-using-virtual-zarr-stores","position":14},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Section 3 - Using Virtual Zarr Stores","lvl2":"Structure"},"content":"The Using Virtual Zarr Stores section contains notebooks demonstrating how to load existing references into Xarray, generating coordinates for GeoTiffs using xrefcoord, and plotting using Hvplot Datashader.","type":"content","url":"/#section-3-using-virtual-zarr-stores","position":15},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":16},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\n\nor on your local machine.","type":"content","url":"/#running-the-notebooks","position":17},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":18},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. You’ll be able to execute\nand even change the example programs. The code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":19},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":20},{"hierarchy":{"lvl1":"Virtual Zarr Cookbook (Kerchunk and VirtualiZarr)","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer,\nyou will need to follow this workflow:\n\nInstall \n\nmambaforge/mamba\n\nClone the https://github.com/ProjectPythia/kerchunk-cookbook repository: git clone https://github.com/ProjectPythia/kerchunk-cookbook.git\n\nMove into the kerchunk-cookbook directorycd kerchunk-cookbook\n\nCreate and activate your conda environment from the environment.yml file.\nNote: In the environment.yml file, Kerchunk` is currently being installed from source as development is happening rapidly.mamba env create -f environment.yml\nmamba activate kerchunk-cookbook\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":21},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references"},"type":"lvl1","url":"/notebooks/advanced/parquet-reference-storage","position":0},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references"},"content":"\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage","position":1},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references"},"type":"lvl1","url":"/notebooks/advanced/parquet-reference-storage#store-virtual-datasets-as-kerchunk-parquet-references","position":2},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references"},"content":"\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#store-virtual-datasets-as-kerchunk-parquet-references","position":3},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/advanced/parquet-reference-storage#overview","position":4},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl2":"Overview"},"content":"In this notebook we will cover how to store virtual datasets as Kerchunk Parquet references instead of Kerchunk JSON references. For large virtual datasets, using Parquet should have performance implications as the overall reference file size should be smaller and the memory overhead of combining the reference files should be lower.\n\nThis notebook builds upon the \n\nKerchunk Basics, \n\nMulti-File Datasets with Kerchunk and the \n\nKerchunk and Dask notebooks.","type":"content","url":"/notebooks/advanced/parquet-reference-storage#overview","position":5},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/advanced/parquet-reference-storage#prerequisites","position":6},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nBasics of virtual Zarr stores\n\nRequired\n\nCore\n\nMulti-file virtual datasets with VirtualiZarr\n\nRequired\n\nCore\n\nParallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask\n\nRequired\n\nCore\n\nIntroduction to Xarray\n\nRequired\n\nIO/Visualization\n\nTime to learn: 30 minutes\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#prerequisites","position":7},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/advanced/parquet-reference-storage#imports","position":8},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl2":"Imports"},"content":"\n\nimport logging\n\nimport dask\nimport fsspec\nimport xarray as xr\nfrom distributed import Client\nfrom virtualizarr import open_virtual_dataset\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#imports","position":9},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl2":"Setting up the Dask Client"},"type":"lvl2","url":"/notebooks/advanced/parquet-reference-storage#setting-up-the-dask-client","position":10},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl2":"Setting up the Dask Client"},"content":"\n\nclient = Client(n_workers=8, silence_logs=logging.ERROR)\nclient\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#setting-up-the-dask-client","position":11},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl2":"Create Input File List"},"type":"lvl2","url":"/notebooks/advanced/parquet-reference-storage#create-input-file-list","position":12},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl2":"Create Input File List"},"content":"Here we are using fsspec's glob functionality along with the * wildcard operator and some string slicing to grab a list of NetCDF files from a s3 fsspec filesystem.\n\n# Initiate fsspec filesystems for reading\nfs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n\nfiles_paths = fs_read.glob(\"s3://smn-ar-wrf/DATA/WRF/DET/2022/12/31/12/*\")\n\n# Here we prepend the prefix 's3://', which points to AWS.\nfiles_paths = sorted([\"s3://\" + f for f in files_paths])\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#create-input-file-list","position":13},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl3":"Subset the Data","lvl2":"Create Input File List"},"type":"lvl3","url":"/notebooks/advanced/parquet-reference-storage#subset-the-data","position":14},{"hierarchy":{"lvl1":"Store virtual datasets as Kerchunk Parquet references","lvl3":"Subset the Data","lvl2":"Create Input File List"},"content":"To speed up our example, lets take a subset of the year of data.\n\n# If the subset_flag == True (default), the list of input files will\n# be subset to speed up the processing\nsubset_flag = True\nif subset_flag:\n    files_paths = files_paths[0:4]\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#subset-the-data","position":15},{"hierarchy":{"lvl1":"Generate Lazy References"},"type":"lvl1","url":"/notebooks/advanced/parquet-reference-storage#generate-lazy-references","position":16},{"hierarchy":{"lvl1":"Generate Lazy References"},"content":"Here we create a function to generate a list of Dask delayed objects.\n\ndef generate_virtual_dataset(file, storage_options):\n    return open_virtual_dataset(\n        file, indexes={}, reader_options={\"storage_options\": storage_options}\n    )\n\n\nstorage_options = dict(anon=True, default_fill_cache=False, default_cache_type=\"first\")\n# Generate Dask Delayed objects\ntasks = [\n    dask.delayed(generate_virtual_dataset)(file, storage_options)\n    for file in files_paths\n]\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#generate-lazy-references","position":17},{"hierarchy":{"lvl1":"Generate Lazy References","lvl3":"Start the Dask Processing"},"type":"lvl3","url":"/notebooks/advanced/parquet-reference-storage#start-the-dask-processing","position":18},{"hierarchy":{"lvl1":"Generate Lazy References","lvl3":"Start the Dask Processing"},"content":"To view the processing you can view it in real-time on the Dask Dashboard. ex: \n\nhttp://​127​.0​.0​.1:8787​/status\n\nvirtual_datasets = list(dask.compute(*tasks))\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#start-the-dask-processing","position":19},{"hierarchy":{"lvl1":"Generate Lazy References","lvl2":"Combine virtual datasets using VirtualiZarr"},"type":"lvl2","url":"/notebooks/advanced/parquet-reference-storage#combine-virtual-datasets-using-virtualizarr","position":20},{"hierarchy":{"lvl1":"Generate Lazy References","lvl2":"Combine virtual datasets using VirtualiZarr"},"content":"\n\ncombined_vds = xr.combine_nested(\n    virtual_datasets, concat_dim=[\"time\"], coords=\"minimal\", compat=\"override\"\n)\ncombined_vds\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#combine-virtual-datasets-using-virtualizarr","position":21},{"hierarchy":{"lvl1":"Generate Lazy References","lvl2":"Write the virtual dataset to a Kerchunk Parquet reference"},"type":"lvl2","url":"/notebooks/advanced/parquet-reference-storage#write-the-virtual-dataset-to-a-kerchunk-parquet-reference","position":22},{"hierarchy":{"lvl1":"Generate Lazy References","lvl2":"Write the virtual dataset to a Kerchunk Parquet reference"},"content":"\n\ncombined_vds.virtualize.to_kerchunk(\"combined.parq\", format=\"parquet\")\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#write-the-virtual-dataset-to-a-kerchunk-parquet-reference","position":23},{"hierarchy":{"lvl1":"Shutdown the Dask cluster"},"type":"lvl1","url":"/notebooks/advanced/parquet-reference-storage#shutdown-the-dask-cluster","position":24},{"hierarchy":{"lvl1":"Shutdown the Dask cluster"},"content":"\n\nclient.shutdown()\n\n","type":"content","url":"/notebooks/advanced/parquet-reference-storage#shutdown-the-dask-cluster","position":25},{"hierarchy":{"lvl1":"Shutdown the Dask cluster","lvl2":"Load kerchunked dataset"},"type":"lvl2","url":"/notebooks/advanced/parquet-reference-storage#load-kerchunked-dataset","position":26},{"hierarchy":{"lvl1":"Shutdown the Dask cluster","lvl2":"Load kerchunked dataset"},"content":"Next we initiate a fsspec ReferenceFileSystem.\nWe need to pass:\n\nThe name of the parquet store\n\nThe remote protocol (This is the protocol of the input file urls)\n\nThe target protocol (file since we saved our parquet store locally).\n\nstorage_options = {\n    \"remote_protocol\": \"s3\",\n    \"skip_instance_cache\": True,\n    \"remote_options\": {\"anon\": True},\n    \"target_protocol\": \"file\",\n    \"lazy\": True,\n}  # options passed to fsspec\nopen_dataset_options = {\"chunks\": {}}  # opens passed to xarray\n\nds = xr.open_dataset(\n    \"combined.parq\",\n    engine=\"kerchunk\",\n    storage_options=storage_options,\n    open_dataset_options=open_dataset_options,\n)\nds","type":"content","url":"/notebooks/advanced/parquet-reference-storage#load-kerchunked-dataset","position":27},{"hierarchy":{"lvl1":"Appending to Kerchunk references"},"type":"lvl1","url":"/notebooks/advanced/appending","position":0},{"hierarchy":{"lvl1":"Appending to Kerchunk references"},"content":"","type":"content","url":"/notebooks/advanced/appending","position":1},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/advanced/appending#overview","position":2},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Overview"},"content":"In this tutorial we’ll show how to append to a pre-existing Kerchunk reference. We’ll use the same datasets as in the \n\nNetCDF reference generation example.","type":"content","url":"/notebooks/advanced/appending#overview","position":3},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/advanced/appending#prerequisites","position":4},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nKerchunk Basics\n\nRequired\n\nCore\n\nMultiple Files and Kerchunk\n\nRequired\n\nCore\n\nMulti-File Datasets with Kerchunk\n\nRequired\n\nIO/Visualization\n\nTime to learn: 45 minutes\n\n","type":"content","url":"/notebooks/advanced/appending#prerequisites","position":5},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/advanced/appending#imports","position":6},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Imports"},"content":"\n\nimport logging\nfrom tempfile import TemporaryDirectory\n\nimport dask\nimport fsspec\nimport ujson\nimport xarray as xr\nfrom distributed import Client\nfrom kerchunk.combine import MultiZarrToZarr\nfrom kerchunk.hdf import SingleHdf5ToZarr\n\n","type":"content","url":"/notebooks/advanced/appending#imports","position":7},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Create Input File List"},"type":"lvl2","url":"/notebooks/advanced/appending#create-input-file-list","position":8},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Create Input File List"},"content":"Here we are using fsspec's glob functionality along with the * wildcard operator and some string slicing to grab a list of NetCDF files from a s3 fsspec filesystem.\n\n# Initiate fsspec filesystems for reading\nfs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n\nfiles_paths = fs_read.glob(\n    \"s3://smn-ar-wrf/DATA/WRF/DET/2022/12/31/12/WRFDETAR_01H_20221231_12_*\"\n)\n\n# Here we prepend the prefix 's3://', which points to AWS.\nfile_pattern = sorted([\"s3://\" + f for f in files_paths])\n\n# This dictionary will be passed as kwargs to `fsspec`. For more details, check out the\n# `foundations/kerchunk_basics` notebook.\nso = dict(mode=\"rb\", anon=True, default_fill_cache=False, default_cache_type=\"first\")\n\n# We are creating a temporary directory to store the .json reference files\n# Alternately, you could write these to cloud storage.\ntd = TemporaryDirectory()\ntemp_dir = td.name\n\n","type":"content","url":"/notebooks/advanced/appending#create-input-file-list","position":9},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Start a Dask Client"},"type":"lvl2","url":"/notebooks/advanced/appending#start-a-dask-client","position":10},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Start a Dask Client"},"content":"To parallelize the creation of our reference files, we will use Dask. For a detailed guide on how to use Dask and Kerchunk, see the Foundations notebook: \n\nKerchunk and Dask.\n\nclient = Client(n_workers=8, silence_logs=logging.ERROR)\nclient\n\n","type":"content","url":"/notebooks/advanced/appending#start-a-dask-client","position":11},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Create a Kerchunk reference file for the first 24 hours"},"type":"lvl2","url":"/notebooks/advanced/appending#create-a-kerchunk-reference-file-for-the-first-24-hours","position":12},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Create a Kerchunk reference file for the first 24 hours"},"content":"\n\nfirst_24_hrs = file_pattern[:24]\n\n# Use Kerchunk's `SingleHdf5ToZarr` method to create a `Kerchunk` index from\n# a NetCDF file.\n\n\ndef generate_json_reference(fil, output_dir: str):\n    with fs_read.open(fil, **so) as infile:\n        h5chunks = SingleHdf5ToZarr(infile, fil, inline_threshold=300)\n        fname = fil.split(\"/\")[-1].strip(\".nc\")\n        outf = f\"{output_dir}/{fname}.json\"\n        with open(outf, \"wb\") as f:\n            f.write(ujson.dumps(h5chunks.translate()).encode())\n        return outf\n\n\n# Generate Dask Delayed objects\ntasks = [dask.delayed(generate_json_reference)(fil, temp_dir) for fil in first_24_hrs]\n\n# Start parallel processing\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\ndask.compute(tasks)\n\n","type":"content","url":"/notebooks/advanced/appending#create-a-kerchunk-reference-file-for-the-first-24-hours","position":13},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Combine .json Kerchunk reference files and write a combined Kerchunk index"},"type":"lvl2","url":"/notebooks/advanced/appending#combine-json-kerchunk-reference-files-and-write-a-combined-kerchunk-index","position":14},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Combine .json Kerchunk reference files and write a combined Kerchunk index"},"content":"In the following cell, we are combining all the .json reference files that were generated above into a single reference file and writing that file to disk.\n\n# Create a list of reference json files\noutput_files = [\n    f\"{temp_dir}/{f.strip('.nc').split('/')[-1]}.json\" for f in first_24_hrs\n]\n\n# combine individual references into single consolidated reference\nmzz = MultiZarrToZarr(\n    output_files,\n    concat_dims=[\"time\"],\n    identical_dims=[\"y\", \"x\"],\n    remote_protocol=\"s3\",\n    remote_options={\"anon\": True},\n    coo_map={\"time\": \"cf:time\"},\n)\n# save translate reference in memory for later visualization\nmulti_kerchunk = mzz.translate()\n\n# Write kerchunk .json record.\noutput_fname = \"ARG_combined.json\"\nwith open(f\"{output_fname}\", \"wb\") as f:\n    f.write(ujson.dumps(multi_kerchunk).encode())\n\n","type":"content","url":"/notebooks/advanced/appending#combine-json-kerchunk-reference-files-and-write-a-combined-kerchunk-index","position":15},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Append references for the next 24 hours"},"type":"lvl2","url":"/notebooks/advanced/appending#append-references-for-the-next-24-hours","position":16},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Append references for the next 24 hours"},"content":"We’ll now append the references for the next 24 hours. First, we create an individual temporary reference file for each input data file. Then,\nwe load the original references and append the new references.\n\n# First generate the individual reference files to be appended\n\nsecond_24_hrs = file_pattern[24:48]\n\n# Generate Dask Delayed objects\ntasks = [dask.delayed(generate_json_reference)(fil, temp_dir) for fil in second_24_hrs]\n\n# Generate reference files for the individual NetCDF files\ndask.compute(tasks)\n\n# Load the original references\nfs_local = fsspec.filesystem(\"file\")\narchive = ujson.load(fs_local.open(output_fname))\n\n# Create a list of individual reference files to append to the combined reference\noutput_files = [\n    f\"{temp_dir}/{f.strip('.nc').split('/')[-1]}.json\" for f in second_24_hrs\n]\n\n# Append to the existing reference file\nmzz = MultiZarrToZarr.append(\n    output_files,\n    original_refs=archive,\n    concat_dims=[\"time\"],\n    identical_dims=[\"y\", \"x\"],\n    remote_protocol=\"s3\",\n    remote_options={\"anon\": True},\n    coo_map={\"time\": \"cf:time\"},\n)\n\nmulti_kerchunk = mzz.translate()\n\n# Write kerchunk .json record.\noutput_fname = \"ARG_combined.json\"\nwith open(f\"{output_fname}\", \"wb\") as f:\n    f.write(ujson.dumps(multi_kerchunk).encode())\n\n","type":"content","url":"/notebooks/advanced/appending#append-references-for-the-next-24-hours","position":17},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Opening Reference Dataset with Fsspec and Xarray"},"type":"lvl2","url":"/notebooks/advanced/appending#opening-reference-dataset-with-fsspec-and-xarray","position":18},{"hierarchy":{"lvl1":"Appending to Kerchunk references","lvl2":"Opening Reference Dataset with Fsspec and Xarray"},"content":"\n\nstorage_options = {\n    \"remote_protocol\": \"s3\",\n    \"skip_instance_cache\": True,\n    \"remote_options\": {\"anon\": True}\n}  # options passed to fsspec\nopen_dataset_options = {\"chunks\": {}}  # opens passed to xarray\n\nds = xr.open_dataset(\n    \"ARG_combined.json\",\n    engine=\"kerchunk\",\n    storage_options=storage_options,\n    open_dataset_options=open_dataset_options,\n)\n\nds\n","type":"content","url":"/notebooks/advanced/appending#opening-reference-dataset-with-fsspec-and-xarray","position":19},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores"},"type":"lvl1","url":"/notebooks/foundations/kerchunk-basics","position":0},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores"},"content":"\n\n","type":"content","url":"/notebooks/foundations/kerchunk-basics","position":1},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores"},"type":"lvl1","url":"/notebooks/foundations/kerchunk-basics#basics-of-virtual-zarr-stores","position":2},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores"},"content":"\n\n","type":"content","url":"/notebooks/foundations/kerchunk-basics#basics-of-virtual-zarr-stores","position":3},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-basics#overview","position":4},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl2":"Overview"},"content":"This notebook is intended as an introduction to creating and using virtual Zarr stores.\nIn this tutorial we will:\n\nScan a single NetCDF4/HDF5 file to create a virtual dataset\n\nLearn how to use the output using Xarray and Zarr.\n\nWhile this notebook only examines using VirtualiZarr and Kerchunk on a single NetCDF file, these libraries can be used to create virtual Zarr datasets from collections of many input files. In the following notebook, we will demonstrate this.\n\n","type":"content","url":"/notebooks/foundations/kerchunk-basics#overview","position":5},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-basics#prerequisites","position":6},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntroduction to Xarray\n\nHelpful\n\nBasic features\n\nTime to learn: 60 minutes\n\n","type":"content","url":"/notebooks/foundations/kerchunk-basics#prerequisites","position":7},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-basics#imports","position":8},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl2":"Imports"},"content":"Here we will import a few Python libraries to help with our data processing.\n\nvirtualizarr will be used to generate the virtual Zarr store\n\nXarray for examining the output dataset\n\nimport xarray as xr\nfrom virtualizarr import open_virtual_dataset\n\n","type":"content","url":"/notebooks/foundations/kerchunk-basics#imports","position":9},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl3":"Define storage_options arguments","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-basics#define-storage-options-arguments","position":10},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl3":"Define storage_options arguments","lvl2":"Imports"},"content":"In the dictionary definition in the next cell, we are defining the options that will be passed to \n\nfsspec.open. Any additional kwargs passed in this dictionary through fsspec.open will pass as kwargs to the file system, in our case s3. The API docs for the s3fs filesystem spec can be found \n\nhere.\n\nIn this example we are passing a few kwargs. In short they are:\n\nanon=True: This is a s3fs kwarg that specifies you are not passing any connection credentials and are connecting to a public bucket.\n\ndefault_fill_cache=False: s3fs kwarg that avoids caching in between chunks of files. This may lower memory usage when reading large files.\n\ndefault_cache_type=\"none\": fsspec kwarg that specifies the caching strategy used by fsspec. In this case, we turn off caching entirely to lower memory usage when only using the information from the file once..\n\nstorage_options = dict(anon=True, default_fill_cache=False, default_cache_type=\"none\")\n\n","type":"content","url":"/notebooks/foundations/kerchunk-basics#define-storage-options-arguments","position":11},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl3":"Virtualize a single NetCDF file","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-basics#virtualize-a-single-netcdf-file","position":12},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl3":"Virtualize a single NetCDF file","lvl2":"Imports"},"content":"Below we will virtualize a NetCDF file stored on the AWS cloud. This dataset is a single time slice of a climate downscaled product for Alaska.\n\nThe steps in the cell below are as follows:\n\nCreate a virtual dataset using open_virtual_dataset\n\nWrite the virtual store as a Kerchunk reference JSON using the to_kerchunk method.\n\n# Input URL to dataset. Note this is a netcdf file stored on s3 (cloud dataset).\nurl = \"s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/WRFDS_2060-01-01.nc\"\n\n\n# Create a virtual dataset using VirtualiZarr.\n# We specify `indexes={}` to avoid creating in-memory pandas indexes for each 1D coordinate, since concatenating with pandas indexes is not yet supported in VirtualiZarr\nvirtual_ds = open_virtual_dataset(\n    url, indexes={}, reader_options={\"storage_options\": storage_options}\n)\n# Write the virtual dataset to disk as a Kerchunk JSON. We could alternative write to a Kerchunk JSON or Icechunk Store.\nvirtual_ds.virtualize.to_kerchunk(\"single_file_kerchunk.json\", format=\"json\")\n\n","type":"content","url":"/notebooks/foundations/kerchunk-basics#virtualize-a-single-netcdf-file","position":13},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl3":"Opening virtual datasets","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-basics#opening-virtual-datasets","position":14},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl3":"Opening virtual datasets","lvl2":"Imports"},"content":"In the section below we will use the previously created Kerchunk reference JSON to open the NetCDF file as if it were a Zarr dataset.\n\n# We once again need to provide information for fsspec to access the remote file\nstorage_options = dict(\n    remote_protocol=\"s3\", remote_options=dict(anon=True), skip_instance_cache=True\n)\n# We will use the \"kerchunk\" engine in `xr.open_dataset` and pass the `storage_options` to the `kerchunk` engine through `backend_kwargs`\nds = xr.open_dataset(\n    \"single_file_kerchunk.json\",\n    engine=\"kerchunk\",\n    backend_kwargs={\"storage_options\": storage_options},\n)\nds\n\n","type":"content","url":"/notebooks/foundations/kerchunk-basics#opening-virtual-datasets","position":15},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl3":"Plot dataset","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-basics#plot-dataset","position":16},{"hierarchy":{"lvl1":"Basics of virtual Zarr stores","lvl3":"Plot dataset","lvl2":"Imports"},"content":"\n\nds.TMAX.plot()\n\nNote that the original .nc file size here is 16.8MB, and the created JSON is 26.5kB. These files also tend to compress very well. As you can see, it the JSON can be written anywhere, and gives us access to the underlying data, reading only the chunks we need from remote without downloading the whole file.","type":"content","url":"/notebooks/foundations/kerchunk-basics#plot-dataset","position":17},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr"},"type":"lvl1","url":"/notebooks/foundations/kerchunk-multi-file","position":0},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr"},"content":"\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file","position":1},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr"},"type":"lvl1","url":"/notebooks/foundations/kerchunk-multi-file#multi-file-virtual-datasets-with-virtualizarr","position":2},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr"},"content":"\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#multi-file-virtual-datasets-with-virtualizarr","position":3},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-multi-file#overview","position":4},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Overview"},"content":"This notebook is intends to build off of the \n\nBasics of virtual Zarr stores.\n\nIn this tutorial we will:\n\nCreate a list of input paths for a collection of NetCDF files stored on the cloud.\n\nCreate virtual datasets for each input datasets\n\nCombine the virtual datasets using combine_nested\n\nRead the combined dataset using \n\nXarray and \n\nfsspec.\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#overview","position":5},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-multi-file#prerequisites","position":6},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nBasics of virtual Zarr stores\n\nRequired\n\nBasic features\n\nIntroduction to Xarray\n\nRecommended\n\nIO\n\nTime to learn: 60 minutes\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#prerequisites","position":7},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Flags"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-multi-file#flags","position":8},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Flags"},"content":"In the section below, set the subset flag to be True (default) or False depending if you want this notebook to process the full file list. If set to True, then a subset of the file list will be processed (Recommended)\n\nsubset_flag = True\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#flags","position":9},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-multi-file#imports","position":10},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Imports"},"content":"In our imports block we are using similar imports to the \n\nBasics of virtual Zarr stores tutorial:\n\nfsspec for reading and writing to remote file systems\n\nvirtualizarr will be used to generate the virtual Zarr store\n\nXarray for examining the output dataset\n\nimport fsspec\nimport xarray as xr\nfrom virtualizarr import open_virtual_dataset\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#imports","position":11},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl3":"Create a File Pattern from a list of  input NetCDF files","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-multi-file#create-a-file-pattern-from-a-list-of-input-netcdf-files","position":12},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl3":"Create a File Pattern from a list of  input NetCDF files","lvl2":"Imports"},"content":"Below we will create a list of input files we want to virtualize. In the \n\nBasics of virtual Zarr stores tutorial, we looked at a single file of climate downscaled data over Southern Alaska. In this example, we will build off of that work and use Kerchunk and VirtualiZarr to combine multiple NetCDF files of this dataset into a virtual dataset that can be read as if it were a Zarr store - without copying any data.\n\nWe use the fsspec s3 filesystem’s glob method to create a list of files matching a file pattern. We supply the base url of s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/, which is pointing to an AWS public bucket, for daily rcp85 ccsm downscaled data for the year 2060. After this base url, we tacked on *, which acts as a wildcard for all the files in the directory. We should expect 365 daily NetCDF files.\n\nFinally, we are appending the string s3:// to the list of return files. This will ensure the list of files we get back are s3 urls and can be read by VirtualiZarr and Kerchunk.\n\n# Initiate fsspec filesystems for reading and writing\nfs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n\n# Retrieve list of available days in archive for the year 2060.\nfiles_paths = fs_read.glob(\"s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/*\")\n\n# Here we prepend the prefix 's3://', which points to AWS.\nfiles_paths = sorted([\"s3://\" + f for f in files_paths])\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#create-a-file-pattern-from-a-list-of-input-netcdf-files","position":13},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl4":"As a quick check, it looks like we have a list 365 file paths, which should be a year of downscaled climte data.","lvl3":"Create a File Pattern from a list of  input NetCDF files","lvl2":"Imports"},"type":"lvl4","url":"/notebooks/foundations/kerchunk-multi-file#as-a-quick-check-it-looks-like-we-have-a-list-365-file-paths-which-should-be-a-year-of-downscaled-climte-data","position":14},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl4":"As a quick check, it looks like we have a list 365 file paths, which should be a year of downscaled climte data.","lvl3":"Create a File Pattern from a list of  input NetCDF files","lvl2":"Imports"},"content":"\n\nprint(f\"{len(files_paths)} file paths were retrieved.\")\n\n# If the subset_flag == True (default), the list of input files will\n# be subset to speed up the processing\nif subset_flag:\n    files_paths = files_paths[0:4]\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#as-a-quick-check-it-looks-like-we-have-a-list-365-file-paths-which-should-be-a-year-of-downscaled-climte-data","position":15},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl3":"Optional: If you want to examine one NetCDF files before creating the Kerchunk index, try uncommenting this code snippet below.","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-multi-file#optional-if-you-want-to-examine-one-netcdf-files-before-creating-the-kerchunk-index-try-uncommenting-this-code-snippet-below","position":16},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl3":"Optional: If you want to examine one NetCDF files before creating the Kerchunk index, try uncommenting this code snippet below.","lvl2":"Imports"},"content":"\n\n## Note: Optional piece of code to view one of the NetCDFs\n\n# import s3fs\n\n# fs = fsspec.filesystem(\"s3\",anon=True)\n# ds = xr.open_dataset(fs.open(file_pattern[0]))\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#optional-if-you-want-to-examine-one-netcdf-files-before-creating-the-kerchunk-index-try-uncommenting-this-code-snippet-below","position":17},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Create virtual datasets for every file in the File_Pattern list"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-multi-file#create-virtual-datasets-for-every-file-in-the-file-pattern-list","position":18},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Create virtual datasets for every file in the File_Pattern list"},"content":"Now that we have a list of NetCDF files, we can use VirtualiZarr to create virtual datasets for each one of these.\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#create-virtual-datasets-for-every-file-in-the-file-pattern-list","position":19},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl3":"Define kwargs for fsspec","lvl2":"Create virtual datasets for every file in the File_Pattern list"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-multi-file#define-kwargs-for-fsspec","position":20},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl3":"Define kwargs for fsspec","lvl2":"Create virtual datasets for every file in the File_Pattern list"},"content":"In the cell below, we are creating a dictionary of kwargs to pass to fsspec and the s3 filesystem. Details on this can be found in the \n\nBasics of virtual Zarr stores tutorial in the Define storage_options arguments section\n\nstorage_options = dict(anon=True, default_fill_cache=False, default_cache_type=\"none\")\n\nIn the cell below, we are reusing some of the functionality from the previous tutorial.\nFirst we are defining a function named: generate_json_reference.\nThis function:\n\nUses an fsspec s3 filesystem to read in a NetCDF from a given url.\n\nGenerates a Kerchunk index using the SingleHdf5ToZarr Kerchunk method.\n\nCreates a simplified filename using some string slicing.\n\nUses the local filesystem created with fsspec to write the Kerchunk index to a .json reference file.\n\nBelow the generate_json_reference function we created, we have a simple for loop that iterates through our list of NetCDF file urls and passes them to our generate_json_reference function, which appends the name of each .json reference file to a list named output_files.\n\nvirtual_datasets = [\n    open_virtual_dataset(\n        filepath, indexes={}, reader_options={\"storage_options\": storage_options}\n    )\n    for filepath in files_paths\n]\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#define-kwargs-for-fsspec","position":21},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Combine virtual datasets"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-multi-file#combine-virtual-datasets","position":22},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Combine virtual datasets"},"content":"After we have generated a virtual dataset for each NetCDF file, we can combine these into a single virtual dataset using Xarray’s combine_nested function.\n\ncombined_vds = xr.combine_nested(\n    virtual_datasets, concat_dim=[\"Time\"], coords=\"minimal\", compat=\"override\"\n)\ncombined_vds\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#combine-virtual-datasets","position":23},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Write combined virtual dataset to a Kerchunk JSON for future use"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-multi-file#write-combined-virtual-dataset-to-a-kerchunk-json-for-future-use","position":24},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Write combined virtual dataset to a Kerchunk JSON for future use"},"content":"If we want to keep the combined reference information in memory as well as write the file to .json, we can run the code snippet below.\n\n# Write kerchunk .json record\noutput_fname = \"combined_kerchunk.json\"\ncombined_vds.virtualize.to_kerchunk(output_fname, format=\"json\")\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#write-combined-virtual-dataset-to-a-kerchunk-json-for-future-use","position":25},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Using the output"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-multi-file#using-the-output","position":26},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Using the output"},"content":"Now that we have built a virtual dataset using VirtualiZarr and Kerchunk, we can read all of those original NetCDF files as if they were a single Zarr dataset.\n\n**Since we saved the combined virtual dataset, this work doesn’t have to be repeated for anyone else to use this dataset. All they need is to pass the reference file storing the virtual dataset to Xarray and it is as if they had a Zarr dataset!\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#using-the-output","position":27},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl3":"Open combined virtual dataset with Kerchunk","lvl2":"Using the output"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-multi-file#open-combined-virtual-dataset-with-kerchunk","position":28},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl3":"Open combined virtual dataset with Kerchunk","lvl2":"Using the output"},"content":"\n\n# We once again need to provide information for fsspec to access the remote file\nstorage_options = dict(\n    remote_protocol=\"s3\", remote_options=dict(anon=True), skip_instance_cache=True\n)\n# We will use the \"kerchunk\" engine in `xr.open_dataset` and pass the `storage_options` to the `kerchunk` engine through `backend_kwargs`\nds = xr.open_dataset(\n    output_fname,\n    engine=\"kerchunk\",\n    backend_kwargs={\"storage_options\": storage_options},\n)\nds\n\n","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#open-combined-virtual-dataset-with-kerchunk","position":29},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Plot a slice of the dataset"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-multi-file#plot-a-slice-of-the-dataset","position":30},{"hierarchy":{"lvl1":"Multi-file virtual datasets with VirtualiZarr","lvl2":"Plot a slice of the dataset"},"content":"Here we are using Xarray to select a single time slice of the dataset and plot a map of snow cover over South East Alaska.\n\nds.isel(Time=0).SNOW.plot()","type":"content","url":"/notebooks/foundations/kerchunk-multi-file#plot-a-slice-of-the-dataset","position":31},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask"},"type":"lvl1","url":"/notebooks/foundations/kerchunk-dask","position":0},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask"},"content":"\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask","position":1},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask"},"type":"lvl1","url":"/notebooks/foundations/kerchunk-dask#parallel-virtual-dataset-creation-with-virtualizarr-kerchunk-and-dask","position":2},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask"},"content":"\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#parallel-virtual-dataset-creation-with-virtualizarr-kerchunk-and-dask","position":3},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#overview","position":4},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Overview"},"content":"In this notebook we will cover:\n\nHow to parallelize the creation of virtual datasets using the Dask library.\n\nThis notebook builds upon the \n\nBasics of virtual Zarr stores and the \n\nMulti-file virtual datasets with VirtualiZarr notebooks. A basic understanding of Dask will be helpful, but is not required. This notebook is not intended as a tutorial for using Dask, but will show how to use Dask to greatly speedup the the generation of virtual datasets.","type":"content","url":"/notebooks/foundations/kerchunk-dask#overview","position":5},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#prerequisites","position":6},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nBasics of virtual Zarr stores\n\nRequired\n\nCore\n\nMulti-file virtual datasets with VirtualiZarr\n\nRequired\n\nCore\n\nIntroduction to Xarray\n\nRecommended\n\nIO/Visualization\n\nIntro to Dask\n\nRecommended\n\nParallel Processing\n\nTime to learn: 45 minutes\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#prerequisites","position":7},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Dask and Parallel Processing"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#dask-and-parallel-processing","position":8},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Dask and Parallel Processing"},"content":"Dask is a Python library for parallel computing. It plays well with Xarray, but can be used in many ways across the Python ecosystem. This notebook is not intended to be a guide for how to use Dask, but just an example of how to use Dask to parallelize some VirtualiZarr and Kerchunk functionality.\n\nIn the previous notebook \n\nMulti-file virtual datasets with VirtualiZarr, we were looking at daily downscaled climate data over South-Eastern Alaska. We created a virtual dataset for each input file using list comprehension in Python.\n\nWith Dask, we can call open_virtual_dataset in parallel, which allows us to create multiple virtual datasets at the same time.\n\nFurther on in this notebook, we will show how using Dask can greatly speed-up the process of creating a virtual datasets.\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#dask-and-parallel-processing","position":9},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Setting up the Dask Client"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#setting-up-the-dask-client","position":10},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Setting up the Dask Client"},"content":"In the code below, we are importing Dask Disributed and creating a client. This is the start of our parallel Kerchunk data processing. We are passing the argument n_workers=8. This will inform the Dask client on some of the resource limitations.\n\nNote: Depending on if you are running on a small machine such as a laptop or a larger compute hub, these resources could be tuned to improve performance.\n\nimport logging\n\nfrom distributed import Client\n\nclient = Client(n_workers=8, silence_logs=logging.ERROR)\nclient\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#setting-up-the-dask-client","position":11},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Binder Specific Setup"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#binder-specific-setup","position":12},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Binder Specific Setup"},"content":"If you are running this tutorial on Binder, the configuration may look slightly different.\n\nOnce you start the client, some information should be returned to you as well as a button that says:\n\nLaunch Dashboard in JupyterLab\n\nOnce you click that button, multiple windows of the Dask dashboard should open up.\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#binder-specific-setup","position":13},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Building off of our Previous Work"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#building-off-of-our-previous-work","position":14},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Building off of our Previous Work"},"content":"In the next section, we will re-use some of the code from the \n\nmulti-file virtual datasets with VirtualiZarr notebook. However, we will modify it slightly to make it compatible with Dask.\n\nThe following two cells should look the same as before.  As a reminder we are importing the required libraries, using fsspec to create a list of our input files and setting up some kwargs for fsspec to use.\n\nimport dask\nimport fsspec\nfrom virtualizarr import open_virtual_dataset\n\n# Initiate fsspec filesystems for reading and writing\nfs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n\n# Retrieve list of available days in archive for the year 2060.\nfiles_paths = fs_read.glob(\"s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/*\")\n\n# Here we prepend the prefix 's3://', which points to AWS.\nfiles_paths = sorted([\"s3://\" + f for f in files_paths])\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#building-off-of-our-previous-work","position":15},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl3":"Subset the Data","lvl2":"Building off of our Previous Work"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-dask#subset-the-data","position":16},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl3":"Subset the Data","lvl2":"Building off of our Previous Work"},"content":"To speed up our example, lets take a subset of the year of data.\n\n# If the subset_flag == True (default), the list of input files will\n# be subset to speed up the processing\nsubset_flag = True\nif subset_flag:\n    files_paths = files_paths[0:4]\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#subset-the-data","position":17},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Dask Specific Changes"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#dask-specific-changes","position":18},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Dask Specific Changes"},"content":"Here is the section of code that will change. Instead of iterating through each input file and using open_virtual_dataset to create the virtual datasets, we are iterating through our input file list and creating Dask Delayed Objects. It is not super important to understand this, but a Dask Delayed Object is lazy, meaning it is not computed eagerly. Once we have iterated through all our input files, we end up with a list of Dask Delayed Objects.\n\nWhen we are ready, we can call dask.compute on this list of delayed objects to create virtual datasets in parallel.\n\ndef generate_virtual_dataset(file, storage_options):\n    return open_virtual_dataset(\n        file, indexes={}, reader_options={\"storage_options\": storage_options}\n    )\n\nstorage_options = dict(anon=True, default_fill_cache=False, default_cache_type=\"none\")\n# Generate Dask Delayed objects\ntasks = [\n    dask.delayed(generate_virtual_dataset)(file, storage_options)\n    for file in files_paths\n]\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#dask-specific-changes","position":19},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Dask Task Graph"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#dask-task-graph","position":20},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Dask Task Graph"},"content":"Once you call dask.compute it can be hard to understand what is happening and how far along the process is at any time. Fortunately, Dask has a built in dashboard to help visualize your progress.","type":"content","url":"/notebooks/foundations/kerchunk-dask#dask-task-graph","position":21},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl3":"Running this notebook locally","lvl2":"Dask Task Graph"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-dask#running-this-notebook-locally","position":22},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl3":"Running this notebook locally","lvl2":"Dask Task Graph"},"content":"When you first initialized the Dask client earlier on, it should have returned some information including an address to the dashboard. For example: http://127.0.0.1:8787/status\n\nBy navigating to this address, you should a Dask dashboard that looks something like this.\n\n\n\n\n\nWhen you call dask.compute(tasks), the dashboard should populate with a bunch of tasks. In the dashboard you can monitor your progress, see how resources are being used as well as well as countless other functionality.\n\n\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#running-this-notebook-locally","position":23},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl3":"Running on Binder","lvl2":"Dask Task Graph"},"type":"lvl3","url":"/notebooks/foundations/kerchunk-dask#running-on-binder","position":24},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl3":"Running on Binder","lvl2":"Dask Task Graph"},"content":"If you are running this example notebook on Binder, the Dask dashboard should look slightly different. Since Binder is running the notebook on another computer, navigating to localhost will give you an error.\nThe Binder specific Dask graph should look something more like this:\n\n\n\n\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#running-on-binder","position":25},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Start the Dask Processing"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#start-the-dask-processing","position":26},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Start the Dask Processing"},"content":"\n\ndask.compute(*tasks)\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#start-the-dask-processing","position":27},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Shut down the Dask cluster"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#shut-down-the-dask-cluster","position":28},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Shut down the Dask cluster"},"content":"\n\nclient.shutdown()\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#shut-down-the-dask-cluster","position":29},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Timing"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#timing","position":30},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Timing"},"content":"To demonstrate how Dask can speed-up your virtual dataset generation, the next section will show the timing of generating reference files with and without Dask.\nFor reference, the timing was run on a large AWS Jupyter-Hub (managed by the fine folks at \n\n2i2c) with ~16 CPU and ~64 GB RAM. It is also important to note that the data is also hosted on AWS.\n\nSerial Virtualization\n\nParallel Virtualization (Dask)\n\n7 min 22 s\n\n36 s\n\nRunning our Dask version on the year of data took only ~36 seconds. In comparison, creating the VirtualiZarr virtual datasets one-by-one took about 7 minutes and 22 seconds.\n\nJust by changing a few lines of code and using Dask, we got our code to run 11x faster. One other detail to note is that there is usually a bit of a delay as Dask builds its task graph before any of the tasks are started. All that to say, you may see even better performance when using Dask, VirtualiZarr, and Kerchunk on larger datasets.\n\nNote: These timings may vary for you. There are many factors that may affect performance, such as:\n\nGeographical location of your compute and the source data\n\nInternet speed\n\nCompute resources, IO limits and # of workers given to Dask\n\nLocation of where to write reference files to (cloud vs local)\n\nThis is meant to be an example of how Dask can be used to speed-up Kerchunk not a detailed benchmark in Kerchunk/Dask performance.\n\n","type":"content","url":"/notebooks/foundations/kerchunk-dask#timing","position":31},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Next Steps"},"type":"lvl2","url":"/notebooks/foundations/kerchunk-dask#next-steps","position":32},{"hierarchy":{"lvl1":"Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask","lvl2":"Next Steps"},"content":"In this notebook we demonstrated how Dask can be used to parallelize the creation of virtual datasets. In the following Case Studies section, we will walk though examples of using VirtualiZarr and Kerchunk with Dask to create virtual cloud-optimized datasets.\n\nAdditionally, if you wish to explore more of Kerchunk's Dask integration, you can try checking out Kerchunk's \n\nauto_dask method, which combines many of the Dask setup steps into a single convenient function.","type":"content","url":"/notebooks/foundations/kerchunk-dask#next-steps","position":33},{"hierarchy":{"lvl1":"GRIB2"},"type":"lvl1","url":"/notebooks/generating-references/grib2","position":0},{"hierarchy":{"lvl1":"GRIB2"},"content":"Generating Kerchunk References from GRIB2 files\n\n\n\n","type":"content","url":"/notebooks/generating-references/grib2","position":1},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/generating-references/grib2#overview","position":2},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Overview"},"content":"Within this notebook, we will cover:\n\nGenerating a list of GRIB2 files on a remote filesystem using fsspec\n\nHow to create reference files of GRIB2 files using Kerchunk\n\nCombining multiple Kerchunk reference files using MultiZarrToZarr\n\nThis notebook shares many similarities with the \n\nMulti-File Datasets with Kerchunk and the \n\nNetCDF/HDF5 Argentinian Weather Dataset Case Study, however this case studies examines another data format and uses kerchunk.scan_grib to create reference files.\n\nThis notebook borrows heavily from this \n\nGIST created by \n\nPeter Marsh.","type":"content","url":"/notebooks/generating-references/grib2#overview","position":3},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/generating-references/grib2#prerequisites","position":4},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nKerchunk Basics\n\nRequired\n\nCore\n\nMultiple Files and Kerchunk\n\nRequired\n\nCore\n\nKerchunk and Dask\n\nRequired\n\nCore\n\nIntroduction to Xarray\n\nRequired\n\nIO/Visualization\n\nTime to learn: 45 minutes\n\n","type":"content","url":"/notebooks/generating-references/grib2#prerequisites","position":5},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Motivation"},"type":"lvl2","url":"/notebooks/generating-references/grib2#motivation","position":6},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Motivation"},"content":"Kerchunk supports multiple input file formats. One of these is GRIB2(GRIdded Information in Binary form), which is a binary file format primary used in meteorology and weather datasets. Similar to NetCDF/HDF5, GRIB2 does not support efficient, parallel access. Using Kerchunk, we can read this legacy format as if it were an ARCO (Analysis-Ready, Cloud-Optimized) data format such as Zarr.\n\n","type":"content","url":"/notebooks/generating-references/grib2#motivation","position":7},{"hierarchy":{"lvl1":"GRIB2","lvl2":"About the Dataset"},"type":"lvl2","url":"/notebooks/generating-references/grib2#about-the-dataset","position":8},{"hierarchy":{"lvl1":"GRIB2","lvl2":"About the Dataset"},"content":"The HRRR is a NOAA real-time 3-km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation. Radar data is assimilated in the HRRR every 15 min over a 1-h period adding further detail to that provided by the hourly data assimilation from the 13km radar-enhanced Rapid Refresh.\nNOAA releases a copy of this dataset via the AWS Registry of Open Data.\n\n","type":"content","url":"/notebooks/generating-references/grib2#about-the-dataset","position":9},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Flags"},"type":"lvl2","url":"/notebooks/generating-references/grib2#flags","position":10},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Flags"},"content":"In the section below, set the subset flag to be True (default) or False depending if you want this notebook to process the full file list. If set to True, then a subset of the file list will be processed (Recommended)\n\nsubset_flag = True\n\n","type":"content","url":"/notebooks/generating-references/grib2#flags","position":11},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/generating-references/grib2#imports","position":12},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Imports"},"content":"\n\nimport glob\nimport logging\nfrom tempfile import TemporaryDirectory\n\nimport dask\nimport fsspec\nimport ujson\nfrom distributed import Client\nfrom kerchunk.combine import MultiZarrToZarr\nfrom kerchunk.grib2 import scan_grib\n\n","type":"content","url":"/notebooks/generating-references/grib2#imports","position":13},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Create Input File List"},"type":"lvl2","url":"/notebooks/generating-references/grib2#create-input-file-list","position":14},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Create Input File List"},"content":"Here we create fsspec files-systems for reading remote files and writing local reference files.\nNext we are using fsspec.glob to retrieve a list of file paths and appending the s3:// prefix to them.\n\n# Initiate fsspec filesystems for reading and writing\nfs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n\n# retrieve list of available days in archive\ndays_available = fs_read.glob(\"s3://noaa-hrrr-bdp-pds/hrrr.*\")\n\n# Read HRRR GRIB2 files from April 19, 2023\nfiles = fs_read.glob(\"s3://noaa-hrrr-bdp-pds/hrrr.20230419/conus/*wrfsfcf01.grib2\")\n\n# Append s3 prefix for filelist\nfiles = sorted([\"s3://\" + f for f in files])\n\n# If the subset_flag == True (default), the list of input files will be subset to\n# speed up the processing\nif subset_flag:\n    files = files[0:2]\n\n","type":"content","url":"/notebooks/generating-references/grib2#create-input-file-list","position":15},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Start a Dask Client"},"type":"lvl2","url":"/notebooks/generating-references/grib2#start-a-dask-client","position":16},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Start a Dask Client"},"content":"To parallelize the creation of our reference files, we will use Dask. For a detailed guide on how to use Dask and Kerchunk, see the Foundations notebook: \n\nKerchunk and Dask.\n\nclient = Client(n_workers=8, silence_logs=logging.ERROR)\nclient\n\n","type":"content","url":"/notebooks/generating-references/grib2#start-a-dask-client","position":17},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Iterate through list of files and create Kerchunk indices as .json reference files"},"type":"lvl2","url":"/notebooks/generating-references/grib2#iterate-through-list-of-files-and-create-kerchunk-indices-as-json-reference-files","position":18},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Iterate through list of files and create Kerchunk indices as .json reference files"},"content":"Each input GRIB2 file contains multiple “messages”, each a measure of some variable on a grid, but with grid dimensions not necessarily compatible with one-another. The filter we create in the first line selects only certain types of messages, and indicated that heightAboveGround will be a coordinate of interest.\n\nWe also write a separate JSON for each of the selected message, since these are the basic component data sets (see the loop over out).\n\nNote: scan_grib does not require a filter and will happily create a reference file for each available grib message. However when combining the grib messages using MultiZarrToZarr it is necessary for the messages to share a coordinate system. Thus to make our lives easier and ensure all reference outputs from scan_grib share a coordinate system we pass a filter argument.\n\nafilter = {\"typeOfLevel\": \"heightAboveGround\", \"level\": [2, 10]}\nso = {\"anon\": True}\n\n# We are creating a temporary directory to store the .json reference files\n# Alternately, you could write these to cloud storage.\ntd = TemporaryDirectory()\ntemp_dir = td.name\ntemp_dir\n\ndef make_json_name(\n    file_url, message_number\n):  # create a unique name for each reference file\n    date = file_url.split(\"/\")[3].split(\".\")[1]\n    name = file_url.split(\"/\")[5].split(\".\")[1:3]\n    return f\"{temp_dir}/{date}_{name[0]}_{name[1]}_message{message_number}.json\"\n\n\ndef gen_json(file_url):\n    out = scan_grib(\n        file_url, storage_options=so, inline_threshold=100, filter=afilter\n    )  # create the reference using scan_grib\n    for i, message in enumerate(\n        out\n    ):  # scan_grib outputs a list containing one reference per grib message\n        out_file_name = make_json_name(file_url, i)  # get name\n        with open(out_file_name, \"w\") as f:\n            f.write(ujson.dumps(message))  # write to file\n\n\n# Generate Dask Delayed objects\ntasks = [dask.delayed(gen_json)(fil) for fil in files]\n\n# Start parallel processing\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\ndask.compute(tasks)\n\n","type":"content","url":"/notebooks/generating-references/grib2#iterate-through-list-of-files-and-create-kerchunk-indices-as-json-reference-files","position":19},{"hierarchy":{"lvl1":"GRIB2","lvl3":"Combine Kerchunk reference .json files","lvl2":"Iterate through list of files and create Kerchunk indices as .json reference files"},"type":"lvl3","url":"/notebooks/generating-references/grib2#combine-kerchunk-reference-json-files","position":20},{"hierarchy":{"lvl1":"GRIB2","lvl3":"Combine Kerchunk reference .json files","lvl2":"Iterate through list of files and create Kerchunk indices as .json reference files"},"content":"We know that four coordinates are identical for every one of our component datasets - they are not functions of valid_time.\n\n# Create a list of reference json files\noutput_files = glob.glob(f\"{temp_dir}/*.json\")\n\n# Combine individual references into single consolidated reference\nmzz = MultiZarrToZarr(\n    output_files,\n    concat_dims=[\"valid_time\"],\n    identical_dims=[\"latitude\", \"longitude\", \"heightAboveGround\", \"step\"],\n)\nmulti_kerchunk = mzz.translate()\n\n","type":"content","url":"/notebooks/generating-references/grib2#combine-kerchunk-reference-json-files","position":21},{"hierarchy":{"lvl1":"GRIB2","lvl3":"Write combined Kerchunk reference file to .json","lvl2":"Iterate through list of files and create Kerchunk indices as .json reference files"},"type":"lvl3","url":"/notebooks/generating-references/grib2#write-combined-kerchunk-reference-file-to-json","position":22},{"hierarchy":{"lvl1":"GRIB2","lvl3":"Write combined Kerchunk reference file to .json","lvl2":"Iterate through list of files and create Kerchunk indices as .json reference files"},"content":"\n\n# Write Kerchunk .json record\noutput_fname = \"HRRR_combined.json\"\nwith open(f\"{output_fname}\", \"wb\") as f:\n    f.write(ujson.dumps(multi_kerchunk).encode())\n\n","type":"content","url":"/notebooks/generating-references/grib2#write-combined-kerchunk-reference-file-to-json","position":23},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Shut down the Dask cluster"},"type":"lvl2","url":"/notebooks/generating-references/grib2#shut-down-the-dask-cluster","position":24},{"hierarchy":{"lvl1":"GRIB2","lvl2":"Shut down the Dask cluster"},"content":"\n\nclient.shutdown()","type":"content","url":"/notebooks/generating-references/grib2#shut-down-the-dask-cluster","position":25},{"hierarchy":{"lvl1":"GeoTIFF"},"type":"lvl1","url":"/notebooks/generating-references/geotiff","position":0},{"hierarchy":{"lvl1":"GeoTIFF"},"content":"Generating virutal datasets from GeoTiff files\n\n\n\n","type":"content","url":"/notebooks/generating-references/geotiff","position":1},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/generating-references/geotiff#overview","position":2},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Overview"},"content":"In this tutorial we will cover:\n\nHow to generate virtual datasets from GeoTIFFs.\n\nCombining virtual datasets.","type":"content","url":"/notebooks/generating-references/geotiff#overview","position":3},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/generating-references/geotiff#prerequisites","position":4},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nBasics of virtual Zarr stores\n\nRequired\n\nCore\n\nMulti-file virtual datasets with VirtualiZarr\n\nRequired\n\nCore\n\nParallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask\n\nRequired\n\nCore\n\nIntroduction to Xarray\n\nRequired\n\nIO/Visualization\n\nTime to learn: 30 minutes\n\n","type":"content","url":"/notebooks/generating-references/geotiff#prerequisites","position":5},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"About the Dataset"},"type":"lvl2","url":"/notebooks/generating-references/geotiff#about-the-dataset","position":6},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"About the Dataset"},"content":"The Finish Meterological Institute (FMI) Weather Radar Dataset is a collection of GeoTIFF files containing multiple radar specific variables, such as rainfall intensity, precipitation accumulation (in 1, 12 and 24 hour increments),  radar reflectivity, radial velocity, rain classification and the cloud top height. It is available through the \n\nAWS public data portal and is updated frequently.\n\nMore details on this dataset can be found \n\nhere.\n\nimport logging\nfrom datetime import datetime\n\nimport dask\nimport fsspec\nimport rioxarray\nimport s3fs\nimport xarray as xr\nfrom distributed import Client\nfrom virtualizarr import open_virtual_dataset\n\n","type":"content","url":"/notebooks/generating-references/geotiff#about-the-dataset","position":7},{"hierarchy":{"lvl1":"GeoTIFF","lvl3":"Examining a Single GeoTIFF File","lvl2":"About the Dataset"},"type":"lvl3","url":"/notebooks/generating-references/geotiff#examining-a-single-geotiff-file","position":8},{"hierarchy":{"lvl1":"GeoTIFF","lvl3":"Examining a Single GeoTIFF File","lvl2":"About the Dataset"},"content":"Before we use Kerchunk to create indices for multiple files, we can load a single GeoTiff file to examine it.\n\n# URL pointing to a single GeoTIFF file\nurl = \"s3://fmi-opendata-radar-geotiff/2023/07/01/FIN-ACRR-3067-1KM/202307010100_FIN-ACRR1H-3067-1KM.tif\"\n\n# Initialize a s3 filesystem\nfs = s3fs.S3FileSystem(anon=True)\n\nxds = rioxarray.open_rasterio(fs.open(url))\n\nxds\n\nxds.isel(band=0).where(xds < 2000).plot()\n\n","type":"content","url":"/notebooks/generating-references/geotiff#examining-a-single-geotiff-file","position":9},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Create Input File List"},"type":"lvl2","url":"/notebooks/generating-references/geotiff#create-input-file-list","position":10},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Create Input File List"},"content":"Here we are using fsspec's glob functionality along with the * wildcard operator and some string slicing to grab a list of GeoTIFF files from a s3 fsspec filesystem.\n\n# Initiate fsspec filesystems for reading\nfs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n\nfiles_paths = fs_read.glob(\n    \"s3://fmi-opendata-radar-geotiff/2023/01/01/FIN-ACRR-3067-1KM/*24H-3067-1KM.tif\"\n)\n# Here we prepend the prefix 's3://', which points to AWS.\nfiles_paths = sorted([\"s3://\" + f for f in files_paths])\n\n","type":"content","url":"/notebooks/generating-references/geotiff#create-input-file-list","position":11},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Start a Dask Client"},"type":"lvl2","url":"/notebooks/generating-references/geotiff#start-a-dask-client","position":12},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Start a Dask Client"},"content":"To parallelize the creation of our reference files, we will use Dask. For a detailed guide on how to use Dask and Kerchunk, see the Foundations notebook: \n\nKerchunk and Dask.\n\nclient = Client(n_workers=8, silence_logs=logging.ERROR)\nclient\n\ndef generate_virtual_dataset(file):\n    storage_options = dict(\n        anon=True, default_fill_cache=False, default_cache_type=\"none\"\n    )\n    vds = open_virtual_dataset(\n        file,\n        indexes={},\n        filetype=\"tiff\",\n        reader_options={\n            \"remote_options\": {\"anon\": True},\n            \"storage_options\": storage_options,\n        },\n    )\n    # Pre-process virtual datasets to extract time step information from the filename\n    subst = file.split(\"/\")[-1].split(\".json\")[0].split(\"_\")[0]\n    time_val = datetime.strptime(subst, \"%Y%m%d%H%M\")\n    vds = vds.expand_dims(dim={\"time\": [time_val]})\n    # Only include the raw data, not the overviews\n    vds = vds[[\"0\"]]\n    return vds\n\n# Generate Dask Delayed objects\ntasks = [dask.delayed(generate_virtual_dataset)(file) for file in files_paths]\n\n# Start parallel processing\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nvirtual_datasets = dask.compute(*tasks)\n\n","type":"content","url":"/notebooks/generating-references/geotiff#start-a-dask-client","position":13},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Combine virtual datasets"},"type":"lvl2","url":"/notebooks/generating-references/geotiff#combine-virtual-datasets","position":14},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Combine virtual datasets"},"content":"\n\ncombined_vds = xr.concat(virtual_datasets, dim=\"time\")\ncombined_vds\n\n","type":"content","url":"/notebooks/generating-references/geotiff#combine-virtual-datasets","position":15},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Shut down the Dask cluster"},"type":"lvl2","url":"/notebooks/generating-references/geotiff#shut-down-the-dask-cluster","position":16},{"hierarchy":{"lvl1":"GeoTIFF","lvl2":"Shut down the Dask cluster"},"content":"\n\nclient.shutdown()","type":"content","url":"/notebooks/generating-references/geotiff#shut-down-the-dask-cluster","position":17},{"hierarchy":{"lvl1":"NetCDF"},"type":"lvl1","url":"/notebooks/generating-references/netcdf","position":0},{"hierarchy":{"lvl1":"NetCDF"},"content":"Generating virtual datasets from NetCDF files\n\n\n\n","type":"content","url":"/notebooks/generating-references/netcdf","position":1},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/generating-references/netcdf#overview","position":2},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Overview"},"content":"Within this notebook, we will cover:\n\nHow to access remote NetCDF data using VirtualiZarr and Kerchunk\n\nCombining multiple virtual datasets\n\nThis notebook shares many similarities with the  \n\nmulti-file virtual datasets with VirtualiZarr notebook. If you are confused on the function of a block of code, please refer there for a more detailed breakdown of what each line is doing.","type":"content","url":"/notebooks/generating-references/netcdf#overview","position":3},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/generating-references/netcdf#prerequisites","position":4},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nBasics of virtual Zarr stores\n\nRequired\n\nCore\n\nMulti-file virtual datasets with VirtualiZarr\n\nRequired\n\nCore\n\nParallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask\n\nRequired\n\nCore\n\nIntroduction to Xarray\n\nRequired\n\nIO/Visualization\n\nTime to learn: 45 minutes\n\n","type":"content","url":"/notebooks/generating-references/netcdf#prerequisites","position":5},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Motivation"},"type":"lvl2","url":"/notebooks/generating-references/netcdf#motivation","position":6},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Motivation"},"content":"NetCDF4/HDF5 is one of the most universally adopted file formats in earth sciences, with support of much of the community as well as scientific agencies, data centers and university labs. A huge amount of legacy data has been generated in this format. Fortunately, using VirtualiZarr and Kerchunk, we can read these datasets as if they were an Analysis-Read Cloud-Optimized (ARCO) format such as Zarr.\n\n","type":"content","url":"/notebooks/generating-references/netcdf#motivation","position":7},{"hierarchy":{"lvl1":"NetCDF","lvl2":"About the Dataset"},"type":"lvl2","url":"/notebooks/generating-references/netcdf#about-the-dataset","position":8},{"hierarchy":{"lvl1":"NetCDF","lvl2":"About the Dataset"},"content":"For this example, we will look at a weather dataset composed of multiple NetCDF files.The SMN-Arg is a WRF deterministic weather forecasting dataset created by the Servicio Meteorológico Nacional de Argentina that covers Argentina as well as many neighboring countries at a 4km spatial resolution.\n\nThe model is initialized twice daily at 00 & 12 UTC with hourly forecasts for variables such as temperature, relative humidity, precipitation, wind direction and magnitude etc. for multiple atmospheric levels.\nThe data is output at hourly intervals with a maximum prediction lead time of 72 hours in NetCDF files.\n\nMore details on this dataset can be found \n\nhere.\n\n","type":"content","url":"/notebooks/generating-references/netcdf#about-the-dataset","position":9},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Flags"},"type":"lvl2","url":"/notebooks/generating-references/netcdf#flags","position":10},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Flags"},"content":"In the section below, set the subset flag to be True (default) or False depending if you want this notebook to process the full file list. If set to True, then a subset of the file list will be processed (Recommended)\n\nsubset_flag = True\n\n","type":"content","url":"/notebooks/generating-references/netcdf#flags","position":11},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/generating-references/netcdf#imports","position":12},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Imports"},"content":"\n\nimport logging\n\nimport dask\nimport fsspec\nimport s3fs\nimport xarray as xr\nfrom distributed import Client\nfrom virtualizarr import open_virtual_dataset\n\n","type":"content","url":"/notebooks/generating-references/netcdf#imports","position":13},{"hierarchy":{"lvl1":"NetCDF","lvl3":"Examining a Single NetCDF File","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/generating-references/netcdf#examining-a-single-netcdf-file","position":14},{"hierarchy":{"lvl1":"NetCDF","lvl3":"Examining a Single NetCDF File","lvl2":"Imports"},"content":"Before we use VirtualiZarr to create virtual datasets for multiple files, we can load a single NetCDF file to examine it.\n\n# URL pointing to a single NetCDF file\nurl = \"s3://smn-ar-wrf/DATA/WRF/DET/2022/12/31/00/WRFDETAR_01H_20221231_00_072.nc\"\n\n# Initialize a s3 filesystem\nfs = s3fs.S3FileSystem(anon=True)\n# Use Xarray to open a remote NetCDF file\nds = xr.open_dataset(fs.open(url), engine=\"h5netcdf\")\n\nHere we see the repr from the Xarray Dataset of a single NetCDF file. From examining the output, we can tell that the Dataset dimensions are ['time','y','x'], with time being only a single step.\nLater, when we use Xarray's combine_nested functionality, we will need to know on which dimensions to concatenate across.\n\n","type":"content","url":"/notebooks/generating-references/netcdf#examining-a-single-netcdf-file","position":15},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Create Input File List"},"type":"lvl2","url":"/notebooks/generating-references/netcdf#create-input-file-list","position":16},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Create Input File List"},"content":"Here we are using fsspec's glob functionality along with the * wildcard operator and some string slicing to grab a list of NetCDF files from a s3 fsspec filesystem.\n\n# Initiate fsspec filesystems for reading\nfs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n\nfiles_paths = fs_read.glob(\"s3://smn-ar-wrf/DATA/WRF/DET/2022/12/31/12/*\")\n\n# Here we prepend the prefix 's3://', which points to AWS.\nfiles_paths = sorted([\"s3://\" + f for f in files_paths])\n\n\n# If the subset_flag == True (default), the list of input files will be subset\n# to speed up the processing\nif subset_flag:\n    files_paths = files_paths[0:8]\n\n","type":"content","url":"/notebooks/generating-references/netcdf#create-input-file-list","position":17},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Start a Dask Client"},"type":"lvl2","url":"/notebooks/generating-references/netcdf#start-a-dask-client","position":18},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Start a Dask Client"},"content":"To parallelize the creation of our reference files, we will use Dask. For a detailed guide on how to use Dask and Kerchunk, see the Foundations notebook: \n\nKerchunk and Dask.\n\nclient = Client(n_workers=8, silence_logs=logging.ERROR)\nclient\n\ndef generate_virtual_dataset(file, storage_options):\n    return open_virtual_dataset(\n        file, indexes={}, reader_options={\"storage_options\": storage_options}\n    )\n\n\nstorage_options = dict(anon=True, default_fill_cache=False, default_cache_type=\"none\")\n# Generate Dask Delayed objects\ntasks = [\n    dask.delayed(generate_virtual_dataset)(file, storage_options)\n    for file in files_paths\n]\n\n# Start parallel processing\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nvirtual_datasets = list(dask.compute(*tasks))\n\n","type":"content","url":"/notebooks/generating-references/netcdf#start-a-dask-client","position":19},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Combine virtual datasets and write a Kerchunk reference JSON to store the virtual Zarr store"},"type":"lvl2","url":"/notebooks/generating-references/netcdf#combine-virtual-datasets-and-write-a-kerchunk-reference-json-to-store-the-virtual-zarr-store","position":20},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Combine virtual datasets and write a Kerchunk reference JSON to store the virtual Zarr store"},"content":"In the following cell, we are combining all the `virtual datasets that were generated above into a single reference file and writing that file to disk.\n\ncombined_vds = xr.combine_nested(\n    virtual_datasets, concat_dim=[\"time\"], coords=\"minimal\", compat=\"override\"\n)\ncombined_vds\n\ncombined_vds.virtualize.to_kerchunk(\"ARG_combined.json\", format=\"json\")\n\n","type":"content","url":"/notebooks/generating-references/netcdf#combine-virtual-datasets-and-write-a-kerchunk-reference-json-to-store-the-virtual-zarr-store","position":21},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Shut down the Dask cluster"},"type":"lvl2","url":"/notebooks/generating-references/netcdf#shut-down-the-dask-cluster","position":22},{"hierarchy":{"lvl1":"NetCDF","lvl2":"Shut down the Dask cluster"},"content":"\n\nclient.shutdown()","type":"content","url":"/notebooks/generating-references/netcdf#shut-down-the-dask-cluster","position":23},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in Project Pythia’s Kerchunk Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree"},"type":"lvl1","url":"/notebooks/using-references/datatree","position":0},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree"},"content":"","type":"content","url":"/notebooks/using-references/datatree","position":1},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/using-references/datatree#overview","position":2},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Overview"},"content":"In this tutorial we are going to use a large collection of pre-generated Kerchunk reference files and open them with Xarray’s new \n\nDataTree functionality. This chapter is heavily inspired by \n\nthis blog post.","type":"content","url":"/notebooks/using-references/datatree#overview","position":3},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl4":"About the Dataset","lvl3":"Overview"},"type":"lvl4","url":"/notebooks/using-references/datatree#about-the-dataset","position":4},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl4":"About the Dataset","lvl3":"Overview"},"content":"This collection of reference files were generated from the \n\nNASA NEX-GDDP-CMIP6 (Global Daily Downscaled Projections) dataset.  A version of this dataset is hosted on s3 as a collection of \n\nNetCDF files.","type":"content","url":"/notebooks/using-references/datatree#about-the-dataset","position":5},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/using-references/datatree#prerequisites","position":6},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nKerchunk Basics\n\nRequired\n\nCore\n\nMultiple Files and Kerchunk\n\nRequired\n\nCore\n\nKerchunk and Dask\n\nRequired\n\nCore\n\nMulti-File Datasets with Kerchunk\n\nRequired\n\nIO/Visualization\n\nXarray-Datatree Overview\n\nRequired\n\nIO\n\nTime to learn: 30 minutes","type":"content","url":"/notebooks/using-references/datatree#prerequisites","position":7},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Motivation"},"type":"lvl3","url":"/notebooks/using-references/datatree#motivation","position":8},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Motivation"},"content":"In total the dataset is roughly 12TB in compressed blob storage, with a single NetCDF file per yearly timestep, per variable. Downloading this entire dataset for analysis on a local machine would difficult to say the least. The collection of Kerchunk reference files for this entire dataset is only 272 Mb, which is about 42,000 times smaller!\n\n","type":"content","url":"/notebooks/using-references/datatree#motivation","position":9},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Imports"},"type":"lvl3","url":"/notebooks/using-references/datatree#imports","position":10},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Imports"},"content":"\n\nimport dask\nimport hvplot.xarray  # noqa\nimport pandas as pd\nimport xarray as xr\nfrom xarray import DataTree\nfrom distributed import Client\nfrom fsspec.implementations.reference import ReferenceFileSystem\n\n","type":"content","url":"/notebooks/using-references/datatree#imports","position":11},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Read the reference catalog"},"type":"lvl3","url":"/notebooks/using-references/datatree#read-the-reference-catalog","position":12},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Read the reference catalog"},"content":"The NASA NEX-GDDP-CMIP6 dataset is organized by GCM, Scenario and Ensemble Member. Each of these Scenario/GCM combinations is represented as a combined reference file, which was created by merging across variables and concatenating along time-steps. All of these references are organized into a simple .csv catalog in the schema:\n\nGCM/Scenario\n\nurl","type":"content","url":"/notebooks/using-references/datatree#read-the-reference-catalog","position":13},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl2":"Organzing with Xarray-Datatree"},"type":"lvl2","url":"/notebooks/using-references/datatree#organzing-with-xarray-datatree","position":14},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl2":"Organzing with Xarray-Datatree"},"content":"Not all of the GCM/Scenario reference datasets have shared spatial coordinates and many of the have slight differences in their calendar and thus time dimension.\nBecause of this, these cannot be combined into a single Xarray-Dataset. Fortunately Xarray-Datatree provides a higher level abstraction where related Xarray-Datasets are organized into a tree structure where each dataset corresponds to a leaf.\n\n# Read the reference catalog into a Pandas DataFrame\ncat_df = pd.read_csv(\n    \"s3://carbonplan-share/nasa-nex-reference/reference_catalog_nested.csv\"\n)\n# Convert the DataFrame into a dictionary\ncatalog = cat_df.set_index(\"ID\").T.to_dict(\"records\")[0]\n\n","type":"content","url":"/notebooks/using-references/datatree#organzing-with-xarray-datatree","position":15},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Load Reference Datasets into Xarray-DataTree","lvl2":"Organzing with Xarray-Datatree"},"type":"lvl3","url":"/notebooks/using-references/datatree#load-reference-datasets-into-xarray-datatree","position":16},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Load Reference Datasets into Xarray-DataTree","lvl2":"Organzing with Xarray-Datatree"},"content":"In the following cell we create a function load_ref_ds, which can be parallelized via Dask to load Kerchunk references into a dictionary of Xarray-Datasets.\n\ndef load_ref_ds(url: str):\n    fs = ReferenceFileSystem(\n        url,\n        remote_protocol=\"s3\",\n        target_protocol=\"s3\",\n        remote_options={\"anon\": True},\n        target_options={\"anon\": True},\n        lazy=True,\n    )\n    return xr.open_dataset(\n        fs.get_mapper(),\n        engine=\"zarr\",\n        backend_kwargs={\"consolidated\": False},\n        chunks={\"time\": 300},\n    )\n\n\ntasks = {id: dask.delayed(load_ref_ds)(url) for id, url in catalog.items()}\n\n","type":"content","url":"/notebooks/using-references/datatree#load-reference-datasets-into-xarray-datatree","position":17},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Use Dask Distributed to load the Xarray-Datasets from Kerchunk reference files","lvl2":"Organzing with Xarray-Datatree"},"type":"lvl3","url":"/notebooks/using-references/datatree#use-dask-distributed-to-load-the-xarray-datasets-from-kerchunk-reference-files","position":18},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Use Dask Distributed to load the Xarray-Datasets from Kerchunk reference files","lvl2":"Organzing with Xarray-Datatree"},"content":"Using Dask, we are loading 164 reference datasets into memory. Since they are are Xarray datasets the coordinates are loaded eagerly, but the underlying data is still lazy.\n\nclient = Client(n_workers=8)\nclient\n\ncatalog_computed = dask.compute(tasks)\n\n","type":"content","url":"/notebooks/using-references/datatree#use-dask-distributed-to-load-the-xarray-datasets-from-kerchunk-reference-files","position":19},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Build an Xarray-Datatree from the dictionary of datasets","lvl2":"Organzing with Xarray-Datatree"},"type":"lvl3","url":"/notebooks/using-references/datatree#build-an-xarray-datatree-from-the-dictionary-of-datasets","position":20},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Build an Xarray-Datatree from the dictionary of datasets","lvl2":"Organzing with Xarray-Datatree"},"content":"\n\ndt = DataTree.from_dict(catalog_computed[0])\n\n","type":"content","url":"/notebooks/using-references/datatree#build-an-xarray-datatree-from-the-dictionary-of-datasets","position":21},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl4":"Accessing the Datatree","lvl3":"Build an Xarray-Datatree from the dictionary of datasets","lvl2":"Organzing with Xarray-Datatree"},"type":"lvl4","url":"/notebooks/using-references/datatree#accessing-the-datatree","position":22},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl4":"Accessing the Datatree","lvl3":"Build an Xarray-Datatree from the dictionary of datasets","lvl2":"Organzing with Xarray-Datatree"},"content":"A Datatree is a collection of related Xarray datasets. We can access individual datasets using UNIX syntax. In the cell below, we will access a single dataset from the datatree.\n\ndt[\"ACCESS-CM2/ssp585\"]\n\n# or\n\ndt[\"ACCESS-CM2\"][\"ssp585\"]\n\n","type":"content","url":"/notebooks/using-references/datatree#accessing-the-datatree","position":23},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl5":"Convert a Datatree node to a Dataset","lvl4":"Accessing the Datatree","lvl3":"Build an Xarray-Datatree from the dictionary of datasets","lvl2":"Organzing with Xarray-Datatree"},"type":"lvl5","url":"/notebooks/using-references/datatree#convert-a-datatree-node-to-a-dataset","position":24},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl5":"Convert a Datatree node to a Dataset","lvl4":"Accessing the Datatree","lvl3":"Build an Xarray-Datatree from the dictionary of datasets","lvl2":"Organzing with Xarray-Datatree"},"content":"\n\ndt[\"ACCESS-CM2\"][\"ssp585\"].to_dataset()\n\n","type":"content","url":"/notebooks/using-references/datatree#convert-a-datatree-node-to-a-dataset","position":25},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl4":"Operations across a Datatree","lvl3":"Build an Xarray-Datatree from the dictionary of datasets","lvl2":"Organzing with Xarray-Datatree"},"type":"lvl4","url":"/notebooks/using-references/datatree#operations-across-a-datatree","position":26},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl4":"Operations across a Datatree","lvl3":"Build an Xarray-Datatree from the dictionary of datasets","lvl2":"Organzing with Xarray-Datatree"},"content":"A Datatree contains a collection of datasets with related coordinates and variables. Using some in-built methods, we can analyze it as if it were a single dataset. Instead of looping through hundreds of Xarray datasets, we can apply operations across the Datatree. In the example below, we will lazily create a time-series.\n\nts = dt.mean(dim=[\"lat\", \"lon\"])\n\n","type":"content","url":"/notebooks/using-references/datatree#operations-across-a-datatree","position":27},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl4":"Visualize a single dataset with HvPlot","lvl3":"Build an Xarray-Datatree from the dictionary of datasets","lvl2":"Organzing with Xarray-Datatree"},"type":"lvl4","url":"/notebooks/using-references/datatree#visualize-a-single-dataset-with-hvplot","position":28},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl4":"Visualize a single dataset with HvPlot","lvl3":"Build an Xarray-Datatree from the dictionary of datasets","lvl2":"Organzing with Xarray-Datatree"},"content":"\n\ndisplay(  # noqa\n    dt[\"ACCESS-CM2/ssp585\"].to_dataset().pr.hvplot(\"lon\", \"lat\", rasterize=True)\n)\n\n","type":"content","url":"/notebooks/using-references/datatree#visualize-a-single-dataset-with-hvplot","position":29},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Shut down the Dask cluster","lvl2":"Organzing with Xarray-Datatree"},"type":"lvl3","url":"/notebooks/using-references/datatree#shut-down-the-dask-cluster","position":30},{"hierarchy":{"lvl1":"Kerchunk and Xarray-Datatree","lvl3":"Shut down the Dask cluster","lvl2":"Organzing with Xarray-Datatree"},"content":"\n\nclient.shutdown()","type":"content","url":"/notebooks/using-references/datatree#shut-down-the-dask-cluster","position":31},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly"},"type":"lvl1","url":"/notebooks/using-references/hvplot-datashader","position":0},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly"},"content":"","type":"content","url":"/notebooks/using-references/hvplot-datashader","position":1},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/using-references/hvplot-datashader#overview","position":2},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Overview"},"content":"This notebook will demonstrate how to use Kerchunk with hvPlot and Datashader to lazily visualize a reference dataset in a streaming fashion.\n\nWe will be building off the references generated through the notebook content from the\n\nPangeo_Forge notebook, so it’s encouraged you first go through that.","type":"content","url":"/notebooks/using-references/hvplot-datashader#overview","position":3},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/using-references/hvplot-datashader#prerequisites","position":4},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nKerchunk Basics\n\nRequired\n\nCore\n\nIntroduction to Xarray\n\nRequired\n\nIO\n\nIntroduction to hvPlot\n\nRequired\n\nData Visualization\n\nIntroduction to Datashader\n\nRequired\n\nBig Data Visualization\n\nTime to learn: 10 minutes\n\n","type":"content","url":"/notebooks/using-references/hvplot-datashader#prerequisites","position":5},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Motivation"},"type":"lvl2","url":"/notebooks/using-references/hvplot-datashader#motivation","position":6},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Motivation"},"content":"Using Kerchunk, we don’t have to create a copy of the data--instead we create a collection of reference files, so that the original data files can be read as if they were Zarr.\n\nThis enables visualization on-the-fly; simply pass in the URL to the dataset and use hvplot.\n\n","type":"content","url":"/notebooks/using-references/hvplot-datashader#motivation","position":7},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl3":"Getting to Know The Data","lvl2":"Motivation"},"type":"lvl3","url":"/notebooks/using-references/hvplot-datashader#getting-to-know-the-data","position":8},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl3":"Getting to Know The Data","lvl2":"Motivation"},"content":"gridMET is a high-resolution daily meteorological dataset covering CONUS from 1979-2023. It is produced by the Climatology Lab at UC Merced. In this example, we are going to look create a virtual Zarr dataset of a derived variable, Burn Index.\n\n","type":"content","url":"/notebooks/using-references/hvplot-datashader#getting-to-know-the-data","position":9},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/using-references/hvplot-datashader#imports","position":10},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Imports"},"content":"\n\nimport hvplot.xarray  # noqa\nimport xarray as xr\n\n","type":"content","url":"/notebooks/using-references/hvplot-datashader#imports","position":11},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Opening the Kerchunk Dataset"},"type":"lvl2","url":"/notebooks/using-references/hvplot-datashader#opening-the-kerchunk-dataset","position":12},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Opening the Kerchunk Dataset"},"content":"Now, it’s a matter of opening the Kerchunk dataset and calling hvplot with the rasterize=True keyword argument.\n\nIf you’re running this notebook locally, try zooming around the map by hovering over the plot and scrolling; it should update fairly quickly. Note, it will not update if you’re viewing this on the docs page online as there is no backend server, but don’t fret because there’s a demo GIF below!\n\n%%timeit -r 1 -n 1\n\n\nstorage_options = {\n    \"remote_protocol\": \"http\",\n    \"skip_instance_cache\": True,\n}  # options passed to fsspec\nopen_dataset_options = {\"chunks\": {}, \"decode_coords\": \"all\"}  # opens passed to xarray\n\nds_kerchunk = xr.open_dataset(\n    \"references/Pangeo_Forge/reference.json\",\n    engine=\"kerchunk\",\n    storage_options=storage_options,\n    **open_dataset_options,\n)\n\ndisplay(ds_kerchunk.hvplot(\"lon\", \"lat\", rasterize=True))  # noqa\n\n\n\n","type":"content","url":"/notebooks/using-references/hvplot-datashader#opening-the-kerchunk-dataset","position":13},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Comparing Against THREDDS"},"type":"lvl2","url":"/notebooks/using-references/hvplot-datashader#comparing-against-thredds","position":14},{"hierarchy":{"lvl1":"Kerchunk, hvPlot, and Datashader: Visualizing datasets on-the-fly","lvl2":"Comparing Against THREDDS"},"content":"Now, we will be repeating the previous cell, but with THREDDS.\n\nNote how the initial load is longer.\n\nIf you’re running the notebook locally (or a demo GIF below), zooming in/out also takes longer to finish buffering as well.\n\n%%timeit -r 1 -n 1\n\n\ndef url_gen(year):\n    return (\n        f\"http://thredds.northwestknowledge.net:8080/thredds/dodsC/MET/bi/bi_{year}.nc\"\n    )\n\n\nyears = list(range(1979, 1980))\nurls_list = [url_gen(year) for year in years]\nnetcdf_ds = xr.open_mfdataset(urls_list, engine=\"netcdf4\")\ndisplay(netcdf_ds.hvplot(\"lon\", \"lat\", rasterize=True))  # noqa\n\n","type":"content","url":"/notebooks/using-references/hvplot-datashader#comparing-against-thredds","position":15},{"hierarchy":{"lvl1":"Load Kerchunked dataset with Xarray"},"type":"lvl1","url":"/notebooks/using-references/xarray","position":0},{"hierarchy":{"lvl1":"Load Kerchunked dataset with Xarray"},"content":"","type":"content","url":"/notebooks/using-references/xarray","position":1},{"hierarchy":{"lvl1":"Load Kerchunked dataset with Xarray","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/using-references/xarray#overview","position":2},{"hierarchy":{"lvl1":"Load Kerchunked dataset with Xarray","lvl2":"Overview"},"content":"Within this notebook, we will cover:\n\nHow to load a Kerchunk pre-generated reference file into Xarray as if it were a Zarr store.","type":"content","url":"/notebooks/using-references/xarray#overview","position":3},{"hierarchy":{"lvl1":"Load Kerchunked dataset with Xarray","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/using-references/xarray#prerequisites","position":4},{"hierarchy":{"lvl1":"Load Kerchunked dataset with Xarray","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nKerchunk Basics\n\nRequired\n\nCore\n\nXarray Tutorial\n\nRequired\n\nCore\n\nTime to learn: 45 minutes\n\n","type":"content","url":"/notebooks/using-references/xarray#prerequisites","position":5},{"hierarchy":{"lvl1":"Load Kerchunked dataset with Xarray","lvl2":"Opening Reference Dataset with Fsspec and Xarray"},"type":"lvl2","url":"/notebooks/using-references/xarray#opening-reference-dataset-with-fsspec-and-xarray","position":6},{"hierarchy":{"lvl1":"Load Kerchunked dataset with Xarray","lvl2":"Opening Reference Dataset with Fsspec and Xarray"},"content":"One way of using our reference dataset is opening it with Xarray. To do this, we will create an fsspec filesystem and pass it to Xarray.\n\n# create an fsspec reference filesystem from the Kerchunk output\nimport fsspec\nimport xarray as xr\n\nfs = fsspec.filesystem(\n    \"reference\",\n    fo=\"references/ARG_combined.json\",\n    remote_protocol=\"s3\",\n    remote_options={\"anon\": True},\n    skip_instance_cache=True,\n)\nm = fs.get_mapper(\"\")\nds = xr.open_dataset(m, engine=\"zarr\", backend_kwargs={\"consolidated\": False})\n\n","type":"content","url":"/notebooks/using-references/xarray#opening-reference-dataset-with-fsspec-and-xarray","position":7},{"hierarchy":{"lvl1":"Load Kerchunked dataset with Xarray","lvl2":"Opening Reference Dataset with Xarray and the Kerchunk Engine"},"type":"lvl2","url":"/notebooks/using-references/xarray#opening-reference-dataset-with-xarray-and-the-kerchunk-engine","position":8},{"hierarchy":{"lvl1":"Load Kerchunked dataset with Xarray","lvl2":"Opening Reference Dataset with Xarray and the Kerchunk Engine"},"content":"As of writing, the latest version of Kerchunk supports opening an reference dataset with Xarray without specifically creating an fsspec filesystem.  This is the same behavior as the example above, just a few less lines of code.\n\nstorage_options = {\n    \"remote_protocol\": \"s3\",\n    \"skip_instance_cache\": True,\n    \"remote_options\": {\"anon\": True}\n}  # options passed to fsspec\nopen_dataset_options = {\"chunks\": {}}  # opens passed to xarray\n\nds = xr.open_dataset(\n    \"references/ARG_combined.json\",\n    engine=\"kerchunk\",\n    storage_options=storage_options,\n    open_dataset_options=open_dataset_options,\n)","type":"content","url":"/notebooks/using-references/xarray#opening-reference-dataset-with-xarray-and-the-kerchunk-engine","position":9},{"hierarchy":{"lvl1":"Use xrefcoord to Generate Coordinates"},"type":"lvl1","url":"/notebooks/using-references/xrefcoord","position":0},{"hierarchy":{"lvl1":"Use xrefcoord to Generate Coordinates"},"content":"","type":"content","url":"/notebooks/using-references/xrefcoord","position":1},{"hierarchy":{"lvl1":"Use xrefcoord to Generate Coordinates"},"type":"lvl1","url":"/notebooks/using-references/xrefcoord#use-xrefcoord-to-generate-coordinates","position":2},{"hierarchy":{"lvl1":"Use xrefcoord to Generate Coordinates"},"content":"When using Kerchunk to generate reference datasets for GeoTIFF’s, only the dimensions are preserved. xrefcoord is a small utility that allows us to generate coordinates for these reference datasets using the geospatial metadata. Similar to other accessor add-on libraries for Xarray such as rioxarray and xwrf, xrefcord provides an accessor for an Xarray dataset. Importing xrefcoord allows us to use the .xref accessor to access additional methods.\n\nIn this tutorial we will use the generate_coords method to build coordinates for the Xarray dataset. xrefcoord is very experimental and makes assumptions about the underlying data, such as each variable shares the same dimensions etc. Use with caution!\n\n","type":"content","url":"/notebooks/using-references/xrefcoord#use-xrefcoord-to-generate-coordinates","position":3},{"hierarchy":{"lvl1":"Overview"},"type":"lvl1","url":"/notebooks/using-references/xrefcoord#overview","position":4},{"hierarchy":{"lvl1":"Overview"},"content":"Within this notebook, we will cover:\n\nHow to load a Kerchunk reference dataset created from a collection of GeoTIFFs\n\nHow to use xrefcoord to generate coordinates from a GeoTIFF reference dataset","type":"content","url":"/notebooks/using-references/xrefcoord#overview","position":5},{"hierarchy":{"lvl1":"Prerequisites"},"type":"lvl1","url":"/notebooks/using-references/xrefcoord#prerequisites","position":6},{"hierarchy":{"lvl1":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nKerchunk Basics\n\nRequired\n\nCore\n\nXarray Tutorial\n\nRequired\n\nCore\n\nTime to learn: 45 minutes\n\nimport xarray as xr\nimport xrefcoord  # noqa\n\nstorage_options = {\n    \"remote_protocol\": \"s3\",\n    \"skip_instance_cache\": True,\n    \"remote_options\": {\"anon\": True}\n}  # options passed to fsspec\nopen_dataset_options = {\"chunks\": {}}  # opens passed to xarray\n\nds = xr.open_dataset(\n    \"references/RADAR.json\",\n    engine=\"kerchunk\",\n    storage_options=storage_options,\n    open_dataset_options=open_dataset_options,\n)\n\n# Generate coordinates from reference dataset\nref_ds = ds.xref.generate_coords(time_dim_name=\"time\", x_dim_name=\"X\", y_dim_name=\"Y\")\n# Rename to rain accumulation in 24 hour period\nref_ds = ref_ds.rename({\"0\": \"rr24h\"})\n\n","type":"content","url":"/notebooks/using-references/xrefcoord#prerequisites","position":7},{"hierarchy":{"lvl1":"Create a Map"},"type":"lvl1","url":"/notebooks/using-references/xrefcoord#create-a-map","position":8},{"hierarchy":{"lvl1":"Create a Map"},"content":"Here we are using Xarray to select a single time slice and create a map of 24 hour accumulated rainfall.\n\nref_ds[\"rr24h\"].where(ref_ds.rr24h < 60000).isel(time=0).plot(robust=True)\n\n","type":"content","url":"/notebooks/using-references/xrefcoord#create-a-map","position":9},{"hierarchy":{"lvl1":"Create a Time-Series"},"type":"lvl1","url":"/notebooks/using-references/xrefcoord#create-a-time-series","position":10},{"hierarchy":{"lvl1":"Create a Time-Series"},"content":"Next we are plotting accumulated rain as a function of time for a specific point.\n\nref_ds[\"rr24h\"][:, 700, 700].plot()","type":"content","url":"/notebooks/using-references/xrefcoord#create-a-time-series","position":11}]}