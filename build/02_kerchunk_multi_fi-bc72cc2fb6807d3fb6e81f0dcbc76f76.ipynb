{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../thumbnail.png\" width=500 alt=\"Kerchunk Logo\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-file virtual datasets with VirtualiZarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is intends to build off of the [Basics of virtual Zarr stores](./01_kerchunk_basics.ipynb).\n",
    "\n",
    "In this tutorial we will:\n",
    "- Create a list of input paths for a collection of NetCDF files stored on the cloud.\n",
    "- Create virtual datasets for each input datasets\n",
    "- Combine the virtual datasets using combine_nested\n",
    "- Read the combined dataset using [`Xarray`](https://docs.xarray.dev/en/stable/) and [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Basics of virtual Zarr stores](./01_kerchunk_basics.ipynb) | Required | Basic features |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro) | Recommended | IO |\n",
    "\n",
    "- **Time to learn**: 60 minutes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flags\n",
    "In the section below, set the `subset` flag to be `True` (default) or `False` depending if you want this notebook to process the full file list. If set to `True`, then a subset of the file list will be processed (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "In our imports block we are using similar imports to the [Basics of virtual Zarr stores tutorial](./01_kerchunk_basics.ipynb):\n",
    "- `fsspec` for reading and writing to remote file systems\n",
    "- `virtualizarr` will be used to generate the virtual Zarr store\n",
    "- `Xarray` for examining the output dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "from virtualizarr import open_virtual_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a File Pattern from a list of  input NetCDF files\n",
    "\n",
    "Below we will create a list of input files we want to virtualize. In the [Basics of virtual Zarr stores tutorial](./01_kerchunk_basics.ipynb), we looked at a single file of climate downscaled data over Southern Alaska. In this example, we will build off of that work and use `Kerchunk` and `VirtualiZarr` to combine multiple NetCDF files of this dataset into a virtual dataset that can be read as if it were a `Zarr` store - without copying any data.\n",
    "\n",
    "We use the `fsspec` `s3` filesystem's *glob* method to create a list of files matching a file pattern. We supply the base url of `s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/`, which is pointing to an `AWS` public bucket, for daily rcp85 ccsm downscaled data for the year 2060. After this base url, we tacked on *`*`*, which acts as a wildcard for all the files in the directory. We should expect 365 daily `NetCDF` files.\n",
    "\n",
    "Finally, we are appending the string `s3://` to the list of return files. This will ensure the list of files we get back are `s3` urls and can be read by `VirtualiZarr` and `Kerchunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fsspec filesystems for reading and writing\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "\n",
    "# Retrieve list of available days in archive for the year 2060.\n",
    "files_paths = fs_read.glob(\"s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/*\")\n",
    "\n",
    "# Here we prepend the prefix 's3://', which points to AWS.\n",
    "files_paths = sorted([\"s3://\" + f for f in files_paths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a quick check, it looks like we have a list 365 file paths, which should be a year of downscaled climte data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(files_paths)} file paths were retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the subset_flag == True (default), the list of input files will\n",
    "# be subset to speed up the processing\n",
    "if subset_flag:\n",
    "    files_paths = files_paths[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: If you want to examine one NetCDF files before creating the `Kerchunk` index, try uncommenting this code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Optional piece of code to view one of the NetCDFs\n",
    "\n",
    "# import s3fs\n",
    "\n",
    "# fs = fsspec.filesystem(\"s3\",anon=True)\n",
    "# ds = xr.open_dataset(fs.open(file_pattern[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create virtual datasets for every file in the `File_Pattern` list\n",
    "\n",
    "Now that we have a list of NetCDF files, we can use `VirtualiZarr` to create virtual datasets for each one of these."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define kwargs for `fsspec`\n",
    "In the cell below, we are creating a dictionary of `kwargs` to pass to `fsspec` and the `s3` filesystem. Details on this can be found in the [Basics of virtual Zarr stores tutorial](./01_kerchunk_basics.ipynb) in the **Define storage_options arguments** section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_options = dict(anon=True, default_fill_cache=False, default_cache_type=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we are reusing some of the functionality from the previous tutorial. \n",
    "First we are defining a function named: `generate_json_reference`. \n",
    "This function:\n",
    "- Uses an `fsspec` `s3` filesystem to read in a `NetCDF` from a given url.\n",
    "- Generates a `Kerchunk` index using the `SingleHdf5ToZarr` `Kerchunk` method.\n",
    "- Creates a simplified filename using some string slicing.\n",
    "- Uses the local filesystem created with `fsspec` to write the `Kerchunk` index to a `.json` reference file.\n",
    "\n",
    "Below the `generate_json_reference` function we created, we have a simple `for` loop that iterates through our list of `NetCDF` file urls and passes them to our `generate_json_reference` function, which appends the name of each `.json` reference file to a list named **output_files**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_datasets = [\n",
    "    open_virtual_dataset(\n",
    "        filepath, indexes={}, reader_options={\"storage_options\": storage_options}\n",
    "    )\n",
    "    for filepath in files_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine virtual datasets\n",
    "\n",
    "After we have generated a virtual dataset for each `NetCDF` file, we can combine these into a single virtual dataset using Xarray's `combine_nested` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vds = xr.combine_nested(\n",
    "    virtual_datasets, concat_dim=[\"Time\"], coords=\"minimal\", compat=\"override\"\n",
    ")\n",
    "combined_vds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write combined virtual dataset to a Kerchunk JSON for future use\n",
    "If we want to keep the combined reference information in memory as well as write the file to `.json`, we can run the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write kerchunk .json record\n",
    "output_fname = \"combined_kerchunk.json\"\n",
    "combined_vds.virtualize.to_kerchunk(output_fname, format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the output\n",
    "\n",
    "Now that we have built a virtual dataset using `VirtualiZarr` and `Kerchunk`, we can read all of those original `NetCDF` files as if they were a single `Zarr` dataset. \n",
    "\n",
    "\n",
    "**Since we saved the combined virtual dataset, this work doesn't have to be repeated for anyone else to use this dataset. All they need is to pass the reference file storing the virtual dataset to `Xarray` and it is as if they had a `Zarr` dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open combined virtual dataset with Kerchunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We once again need to provide information for fsspec to access the remote file\n",
    "storage_options = dict(\n",
    "    remote_protocol=\"s3\", remote_options=dict(anon=True), skip_instance_cache=True\n",
    ")\n",
    "# We will use the \"kerchunk\" engine in `xr.open_dataset` and pass the `storage_options` to the `kerchunk` engine through `backend_kwargs`\n",
    "ds = xr.open_dataset(\n",
    "    output_fname,\n",
    "    engine=\"kerchunk\",\n",
    "    backend_kwargs={\"storage_options\": storage_options},\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a slice of the dataset\n",
    "\n",
    "Here we are using `Xarray` to select a single time slice of the dataset and plot a map of snow cover over South East Alaska."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(Time=0).SNOW.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerchunk-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
