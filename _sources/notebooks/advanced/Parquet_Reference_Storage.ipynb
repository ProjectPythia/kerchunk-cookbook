{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/47/Apache_Parquet_logo.svg\" width=400 alt=\"Parquet Logo\"></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store virtual datasets as Kerchunk Parquet references"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "   \n",
    "In this notebook we will cover how to store virtual datasets as Kerchunk Parquet references instead of Kerchunk JSON references. For large virtual datasets, using Parquet should have performance implications as the overall reference file size should be smaller and the memory overhead of combining the reference files should be lower. \n",
    "\n",
    "\n",
    "This notebook builds upon the [Kerchunk Basics](notebooks/foundations/01_kerchunk_basics.ipynb), [Multi-File Datasets with Kerchunk](notebooks/foundations/02_kerchunk_multi_file.ipynb) and the [Kerchunk and Dask](notebooks/foundations/03_kerchunk_dask.ipynb) notebooks. \n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Basics of virtual Zarr stores](../foundations/01_kerchunk_basics.ipynb) | Required | Core |\n",
    "| [Multi-file virtual datasets with VirtualiZarr](../foundations/02_kerchunk_multi_file.ipynb) | Required | Core |\n",
    "| [Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask](../foundations/03_kerchunk_dask) | Required | Core |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Required | IO/Visualization |\n",
    "\n",
    "- **Time to learn**: 30 minutes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import dask\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "from distributed import Client\n",
    "from virtualizarr import open_virtual_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the `Dask` Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=8, silence_logs=logging.ERROR)\n",
    "client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Input File List\n",
    "\n",
    "Here we are using `fsspec's` glob functionality along with the *`*`* wildcard operator and some string slicing to grab a list of NetCDF files from a `s3` `fsspec` filesystem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fsspec filesystems for reading\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "\n",
    "files_paths = fs_read.glob(\"s3://smn-ar-wrf/DATA/WRF/DET/2022/12/31/12/*\")\n",
    "\n",
    "# Here we prepend the prefix 's3://', which points to AWS.\n",
    "files_paths = sorted([\"s3://\" + f for f in files_paths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset the Data\n",
    "To speed up our example, lets take a subset of the year of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the subset_flag == True (default), the list of input files will\n",
    "# be subset to speed up the processing\n",
    "subset_flag = True\n",
    "if subset_flag:\n",
    "    files_paths = files_paths[0:4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Lazy References\n",
    "\n",
    "Here we create a function to generate a list of Dask delayed objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_virtual_dataset(file, storage_options):\n",
    "    return open_virtual_dataset(\n",
    "        file, indexes={}, reader_options={\"storage_options\": storage_options}\n",
    "    )\n",
    "\n",
    "\n",
    "storage_options = dict(anon=True, default_fill_cache=False, default_cache_type=\"first\")\n",
    "# Generate Dask Delayed objects\n",
    "tasks = [\n",
    "    dask.delayed(generate_virtual_dataset)(file, storage_options)\n",
    "    for file in files_paths\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Dask Processing\n",
    "To view the processing you can view it in real-time on the Dask Dashboard. ex: http://127.0.0.1:8787/status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_datasets = list(dask.compute(*tasks))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine virtual datasets using VirtualiZarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vds = xr.combine_nested(\n",
    "    virtual_datasets, concat_dim=[\"time\"], coords=\"minimal\", compat=\"override\"\n",
    ")\n",
    "combined_vds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the virtual dataset to a Kerchunk Parquet reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vds.virtualize.to_kerchunk(\"combined.parq\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shutdown the Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load kerchunked dataset\n",
    "\n",
    "Next we initiate a `fsspec` `ReferenceFileSystem`.\n",
    "We need to pass:\n",
    "- The name of the parquet store\n",
    "- The remote protocol (This is the protocol of the input file urls)\n",
    "- The target protocol (`file` since we saved our parquet store locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_options = {\n",
    "    \"remote_protocol\": \"s3\",\n",
    "    \"skip_instance_cache\": True,\n",
    "    \"remote_options\": {\"anon\": True},\n",
    "    \"target_protocol\": \"file\",\n",
    "    \"lazy\": True,\n",
    "}  # options passed to fsspec\n",
    "open_dataset_options = {\"chunks\": {}}  # opens passed to xarray\n",
    "\n",
    "ds = xr.open_dataset(\n",
    "    \"combined.parq\",\n",
    "    engine=\"kerchunk\",\n",
    "    storage_options=storage_options,\n",
    "    open_dataset_options=open_dataset_options,\n",
    ")\n",
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerchunk-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
