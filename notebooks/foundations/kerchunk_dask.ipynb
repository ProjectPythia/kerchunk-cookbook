{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../images/dask.png\" width=200 alt=\"Dask Logo\"></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerchunk and Dask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "   \n",
    "In this notebook we will cover:\n",
    "\n",
    "1. How to parallelize the creation of `Kerchunk` reference files using the `Dask` library.\n",
    "1. How to scale up the creation of large `Kerchunk` datasets using `Dask`. \n",
    "\n",
    "This notebook builds upon the [Kerchunk Basics](./kerchunk_basics.ipynb) and the [Multi-File Datasets with Kerchunk](../foundations/kerchunk_multi_file.ipynb) notebooks. A basic understanding of `Dask` will be helpful, but is not required. This notebook is not intended as a tutorial for using `Dask`, but will show how to use `Dask` to greatly speedup the the generation of `Kerchunk` reference files. \n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](../foundations/kerchunk_basics) | Required | Core |\n",
    "| [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file) | Required | Core |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Recommended | IO/Visualization |\n",
    "| [Intro to Dask](https://tutorial.dask.org/00_overview.html) | Recommended | Parallel Processing |\n",
    "\n",
    "- **Time to learn**: 45 minutes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask and Parallel Processing\n",
    "\n",
    "Dask is a `Python` library for parallel computing. It plays well with `Xarray`, but can be used in many ways across the `Python` ecosystem. This notebook is not intended to be a guide for how to use `Dask`, but just an example of how to use `Dask` to parallelize some `Kerchunk` functionality. \n",
    "\n",
    "In the previous notebook [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file), we were looking at daily downscaled climate data over South-Eastern Alaska. In our function named `generate_json_reference`, we were looping over, one at a time, the input NetCDF4 files and using `Kerchunk's` `SingleHdf5ToZarr` method to create a `Kerchunk` index for each file.\n",
    "\n",
    "With `Dask`, we can call `SingleHdf5ToZarr` in parallel, which allows us to create multiple `Kerchunk` reference files at the same time.\n",
    "\n",
    "Further on in this notebook, we will show how using `Dask` can greatly speed-up the process of creating a virtual dataset using `Kerchunk`\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the `Dask` Client\n",
    "\n",
    "In the code below, we are importing `Dask Disributed` and creating a `client`. This is the start of our parallel `Kerchunk` data processing. We are passing the argument `n_workers=8`. This will inform the `Dask` client on some of the resource limitations. \n",
    "\n",
    "Note: Depending on if you are running on a small machine such as a laptop or a larger compute hub, these resources could be tuned to improve performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from distributed import Client\n",
    "\n",
    "client = Client(n_workers=8, silence_logs=logging.ERROR)\n",
    "client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binder Specific Setup\n",
    "\n",
    "If you are running this tutorial on `Binder`, the configuration may look slightly different. \n",
    "\n",
    "Once you start the client, some information should be returned to you as well as a button that says:\n",
    "\n",
    "**Launch Dashboard in JupyterLab**\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/client_binder.png\" width=\"600\"/> \n",
    "  \n",
    "\n",
    "Once you click that button, multiple windows of the `Dask` dashboard should open up. \n",
    "\n",
    "\n",
    "<img src=\"../images/binder_client2.png\" width=\"600\"/> \n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building off of our Previous Work\n",
    "In the next section, we will re-use some of the code from [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file) notebook. However, we will modify it slightly to make it compatable with `Dask`.\n",
    "\n",
    "The following two cells should look the same as before.  As a reminder we are importing the required libraries, using `fsspec` to create a list of our input files and setting up some kwargs for `fsspec` to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import dask\n",
    "import fsspec\n",
    "import ujson\n",
    "import xarray as xr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.hdf import SingleHdf5ToZarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fsspec filesystems for reading and writing\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "fs_write = fsspec.filesystem(\"\")\n",
    "\n",
    "# Retrieve list of available days in archive for the year 2060.\n",
    "files_paths = fs_read.glob(\"s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/*\")\n",
    "\n",
    "# Here we prepend the prefix 's3://', which points to AWS.\n",
    "file_pattern = sorted([\"s3://\" + f for f in files_paths])\n",
    "\n",
    "# Define kwargs for `fsspec`\n",
    "so = dict(mode=\"rb\", anon=True, default_fill_cache=False, default_cache_type=\"first\")\n",
    "\n",
    "\n",
    "# We are creating a temporary directory to store the .json reference files\n",
    "# Alternately, you could write these to cloud storage.\n",
    "td = TemporaryDirectory()\n",
    "temp_dir = td.name\n",
    "temp_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset the Data\n",
    "To speed up our example, lets take a subset of the year of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = file_pattern[0:40]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Specific Changes\n",
    "\n",
    "Here is the section of code that will change. Instead of iterating through each input file and using `generate_json_reference` to create the `Kerchunk` reference files, we are iterating through our input file list and creating `Dask Delayed Objects`. It is not super important to understand this, but a `Dask Delayed Object` is lazy, meaning it is not computed eagerly. Once we have iterated through all our input files, we end up with a list of `Dask Delayed Objects`. \n",
    "\n",
    "When we are ready, we can call `dask.compute` on this list of delayed objects to create  `Kerchunk` reference files in parallel. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kerchunk's `SingleHdf5ToZarr` method to create a `Kerchunk` index from a NetCDF file.\n",
    "def generate_json_reference(fil, temp_dir: str):\n",
    "    with fs_read.open(fil, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, fil, inline_threshold=300)\n",
    "        fname = fil.split(\"/\")[-1].strip(\".nc\")\n",
    "        outf = f\"{temp_dir}/{fname}.json\"\n",
    "        with open(outf, \"wb\") as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode())\n",
    "        return outf\n",
    "\n",
    "\n",
    "# Generate Dask Delayed objects\n",
    "tasks = [dask.delayed(generate_json_reference)(fil, temp_dir) for fil in file_pattern]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Task Graph\n",
    "\n",
    "Once you call `dask.compute` it can be hard to understand what is happening and how far along the process is at any time. Fortunately, `Dask` has a built in dashboard to help visualize your progress.\n",
    "\n",
    "### Running this notebook locally\n",
    "When you first initialized the `Dask` `client` earlier on, it should have returned some information including an address to the `dashboard`. For example: `http://127.0.0.1:8787/status`\n",
    "\n",
    "\n",
    "By navigating to this address, you should a `Dask` dashboard that looks something like this.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"../images/empty_dashboard.png\" width=\"700\"/> \n",
    "  \n",
    "<br/><br/>\n",
    "\n",
    "When you call `dask.compute(tasks)`, the dashboard should populate with a bunch of tasks. In the dashboard you can monitor your progress, see how resources are being used as well as well as countless other functionality. \n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"../images/dashboard.png\"  width=\"700\"/> \n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "### Running on Binder\n",
    "\n",
    "If you are running this example notebook on `Binder`, the `Dask` dashboard should look slightly different. Since `Binder` is running the notebook on another computer, navigating to localhost will give you an error. \n",
    "The `Binder` specific `Dask` graph should look something more like this:\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"../images/task_stream_full.png\"  width=\"700\"/> \n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Dask Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start parallel computation of `SingleHDF5ToZarr`\n",
    "dask.compute(tasks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing\n",
    "\n",
    "\n",
    "| Serial Kerchunk      | Parallel Kerchunk (Dask) |\n",
    "| -------------------- | ------------------------ |\n",
    "| 225 seconds per file | 54 seconds per file      |\n",
    "\n",
    "\n",
    "\n",
    "Running our `Dask` version on a subset of 40 files took about 36 minutes, or 54 second per file. In comparison, computing the kerchunk indicies one-by-one in [Multi-File Datasets with Kerchunk](../foundations/kerchunk_multi_file.ipynb) took about 3 minutes and 45 seconds for each file. \n",
    "\n",
    "\n",
    "\n",
    "Just by changing a few lines of code and using `Dask`, we got our code to run over **4x faster**. One other detail to note is that there is usually a bit of a delay as `Dask` builds its task graph before any of the tasks are started. All that to say, you may see even better performance when using `Dask` and `Kerchunk` on larger datasets.\n",
    "\n",
    "Note: These timings may vary for you. There are many factors that may affect performance, such as:\n",
    "- Geographical location of your compute and the source data\n",
    "- Internet speed\n",
    "- Compute resources, IO limits and # of workers given to `Dask`\n",
    "- Location of where to write reference files to (cloud vs local)\n",
    "- Type of reference files (`Parquet` vs `.JSON`)\n",
    "\n",
    "This is meant to be an example of how `Dask` can be used to speed-up `Kerchunk` not a detailed benchmark in `Kerchunk/Dask` performance. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this notebook we demonstrated how `Dask` can be used to parallelize the creation of `Kerchunk` reference files. In the following `Case Studies` section, we will walk though examples of using `Kerchunk` with `Dask` to create virtual cloud-optimized datasets. \n",
    "\n",
    "Additionally, if you wish to explore more of `Kerchunk's` `Dask` integration, you can try checking out `Kerchunk's` [auto_dask](https://fsspec.github.io/kerchunk/reference.html?highlight=auto%20dask#kerchunk.combine.auto_dask) method, which combines many of the `Dask` setup steps into a single convenient function. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerchunk-cookbook-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "89095a95fbc59e1db286735bee0073a08e46abd63daa66f53634eb5c8cc2192a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
