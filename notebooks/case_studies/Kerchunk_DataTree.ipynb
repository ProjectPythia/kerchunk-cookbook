{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerchunk and Xarray-Datatree at Scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the previous tutorials we have demonstrated how to use Kerchunk on small collections of files. In this tutorial we are going to use a large collection of pre-generated Kerchunk reference files and open them with [xarray-datatree](https://xarray-datatree.readthedocs.io/en/latest/).\n",
    "\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "This collection of reference files were generated from the [**NASA NEX-GDDP-CMIP6 (Global Daily Downscaled Projections)**](https://www.nasa.gov/nex/gddp/) dataset.  A version of this dataset is hosted on `s3` as a collection of NetCDF files. \n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](../foundations/kerchunk_basics) | Required | Core |\n",
    "| [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file) | Required | Core |\n",
    "| [Kerchunk and Dask](../foundations/kerchunk_dask) | Required | Core |\n",
    "| [Multi-File Datasets with Kerchunk](../case_studies/ARG_Weather.ipynb) | Required | IO/Visualization |\n",
    "| [Xarray-Datatree Overview](https://xarray-datatree.readthedocs.io/en/latest/quick-overview.html)| Required | IO |\n",
    "\n",
    "- **Time to learn**: 30 minutes\n",
    "\n",
    "## Motivation\n",
    "\n",
    "In total the dataset is roughly **12TB**, with a single NetCDF file per yearly timestep, per variable. Downloading this entire dataset for analysis on a local machine would difficult to say the least. The collection of Kerchunk reference files for this entire dataset is only **272 Mb**, which is about 42,000 times smaller! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatree import DataTree\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import fsspec\n",
    "import dask\n",
    "from distributed import Client\n",
    "from fsspec.implementations.reference import LazyReferenceMapper, ReferenceFileSystem\n",
    "\n",
    "import hvplot\n",
    "import hvplot.xarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Reference Catalog\n",
    "\n",
    "The **NASA NEX-GDDP-CMIP6** dataset is organized by GCM, Scenario and Ensemble Member. Each of these Scenario/GCM combinations is represented as a combined reference file, which was created by merging across variables and concatenating along time-steps. All of these references are organized into a simple .csv catalog in the schema: \n",
    "|   GCM_Scenario  |      url    |\n",
    "| --------------- | ----------- |\n",
    "\n",
    "\n",
    "# Organzing with Xarray-Datatree\n",
    "Not all of the GCM/Scenario reference datasets have shared spatial coordinates and many of the have slight differences in their calendar and thus time dimension. \n",
    "Because of this, these cannot be combined into a single `Xarray-Dataset`. Fortunatly `Xarray-Datatree` provides a higher level abstraction where related `Xarray-Datasets` are organized into a tree structure where each dataset corresponds to a `leaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reference catalog into a Pandas DataFrame\n",
    "cat_df = pd.read_csv(\n",
    "    \"s3://carbonplan-share/nasa-nex-reference/reference_catalog_prod.csv\"\n",
    ")\n",
    "\n",
    "# Convert the DataFrame into a dictionary\n",
    "catalog = cat_df.set_index(\"ID\").T.to_dict(\"records\")[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reference Datasets into Xarray-DataTree\n",
    "\n",
    "In the following cell we create a function `load_ref_ds`, which can be parallealized via Dask to load Kerchunk references into a dictionary of `Xarray-Datasets`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ref_ds(url: str):\n",
    "\n",
    "    fs = ReferenceFileSystem(\n",
    "        url,\n",
    "        remote_protocol=\"s3\",\n",
    "        target_protocol=\"s3\",\n",
    "        target_options={\"anon\": True},\n",
    "        lazy=True,\n",
    "    )\n",
    "    return xr.open_dataset(\n",
    "        fs.get_mapper(), engine=\"zarr\", backend_kwargs={\"consolidated\": False}\n",
    "    )\n",
    "\n",
    "\n",
    "tasks = {id: dask.delayed(load_ref_ds)(url) for id, url in catalog.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Dask Distributed to load the Xarray-Datasets from Kerchunk reference files\n",
    "Using Dask, we're loading 164 reference datasets into memory. Since they're are Xarray datasets the coordinates are loaded eagerly, but the underlying data is still lazy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=8)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_computed = dask.compute(tasks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Xarray-Datatree from the dictionary of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DataTree.from_dict(catalog_computed[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine Xarray-Datatree Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of data in tree = {dt.nbytes / 1e12 :.2f} TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a single dataset with HvPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dt[\"ACCESS-CM2_ssp585\"].to_dataset().pr.hvplot(\"lon\", \"lat\", rasterize=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
