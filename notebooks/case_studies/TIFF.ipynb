{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerchunk, GeoTIFF and Generating Coordinates with `xrefcoord`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/radar.png\" width=400 alt=\"ARG\"></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial we will cover:\n",
    "\n",
    "1. How to generate `Kerchunk` references of GeoTIFFs.\n",
    "1. Combining `Kerchunk` references into a virtual dataset.\n",
    "1. Generating Coordinates with the `xrefcoord` accessor.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](../foundations/kerchunk_basics) | Required | Core |\n",
    "| [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file) | Required | Core |\n",
    "| [Kerchunk and Dask](../foundations/kerchunk_dask) | Required | Core |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Required | IO/Visualization |\n",
    "- **Time to learn**: 30 minutes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "\n",
    "The Finish Meterological Institute (FMI) Weather Radar Dataset is a collection of GeoTIFF files containing multiple radar specific variables, such as rainfall intensity, precipitation accumulation (in 1, 12 and 24 hour increments),  radar reflectivity, radial velocity, rain classification and the cloud top height. It is available through the [AWS public data portal](https://aws.amazon.com/marketplace/pp/prodview-koodet467asui?sr=0-19&ref_=beagle&applicationId=AWSMPContessa) and is updated frequently. \n",
    "\n",
    "\n",
    "\n",
    "More details on this dataset can be found [here](https://en.ilmatieteenlaitos.fi/radar-data-on-aws-s3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import dask\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import s3fs\n",
    "import ujson\n",
    "import xarray as xr\n",
    "import xrefcoord  # noqa\n",
    "from distributed import Client\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.tiff import tiff_to_zarr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining a Single GeoTIFF File\n",
    "\n",
    "Before we use `Kerchunk` to create indices for multiple files, we can load a single GeoTiff file to examine it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL pointing to a single GeoTIFF file\n",
    "url = \"s3://fmi-opendata-radar-geotiff/2023/07/01/FIN-ACRR-3067-1KM/202307010100_FIN-ACRR1H-3067-1KM.tif\"\n",
    "\n",
    "# Initialize a s3 filesystem\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "xds = rioxarray.open_rasterio(fs.open(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds.isel(band=0).where(xds < 2000).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Input File List\n",
    "\n",
    "Here we are using `fsspec's` glob functionality along with the *`*`* wildcard operator and some string slicing to grab a list of GeoTIFF files from a `s3` `fsspec` filesystem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fsspec filesystems for reading\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "\n",
    "files_paths = fs_read.glob(\n",
    "    \"s3://fmi-opendata-radar-geotiff/2023/01/01/FIN-ACRR-3067-1KM/*24H-3067-1KM.tif\"\n",
    ")\n",
    "# Here we prepend the prefix 's3://', which points to AWS.\n",
    "file_pattern = sorted([\"s3://\" + f for f in files_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary will be passed as kwargs to `fsspec`. For more details, check out the `foundations/kerchunk_basics` notebook.\n",
    "so = dict(mode=\"rb\", anon=True, default_fill_cache=False, default_cache_type=\"first\")\n",
    "\n",
    "# We are creating a temporary directory to store the .json reference files\n",
    "# Alternately, you could write these to cloud storage.\n",
    "td = TemporaryDirectory()\n",
    "temp_dir = td.name\n",
    "temp_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a Dask Client\n",
    "\n",
    "To parallelize the creation of our reference files, we will use `Dask`. For a detailed guide on how to use Dask and Kerchunk, see the Foundations notebook: [Kerchunk and Dask](../foundations/kerchunk_dask).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=8, silence_logs=logging.ERROR)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kerchunk's `tiff_to_zarr` method to create create reference files\n",
    "\n",
    "\n",
    "def generate_json_reference(fil, output_dir: str):\n",
    "    tiff_chunks = tiff_to_zarr(\n",
    "        fil, remote_options={\"protocol\": \"s3\"} | so, target_options=so\n",
    "    )\n",
    "    fname = fil.split(\"/\")[-1].split(\"_\")[0]\n",
    "    outf = f\"{output_dir}/{fname}.json\"\n",
    "    with open(outf, \"wb\") as f:\n",
    "        f.write(ujson.dumps(tiff_chunks).encode())\n",
    "    return outf\n",
    "\n",
    "\n",
    "# Generate Dask Delayed objects\n",
    "tasks = [dask.delayed(generate_json_reference)(fil, temp_dir) for fil in file_pattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start parallel processing\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "dask.compute(tasks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Reference Files into Multi-File Reference Dataset\n",
    "\n",
    "Now we will combine all the reference files generated into a single reference dataset. Since each TIFF file is a single timeslice and the only temporal information is stored in the filepath, we will have to specify the `coo_map` kwarg in `MultiZarrToZarr` to build a dimension from the filepath attributes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_files = sorted(glob.iglob(f\"{temp_dir}/*.json\"))\n",
    "\n",
    "\n",
    "# Custom Kerchunk function from `coo_map` to create dimensions\n",
    "def fn_to_time(index, fs, var, fn):\n",
    "    import datetime\n",
    "    import re\n",
    "\n",
    "    subst = fn.split(\"/\")[-1].split(\".json\")[0]\n",
    "    return datetime.datetime.strptime(subst, \"%Y%m%d%H%M\")\n",
    "\n",
    "\n",
    "mzz = MultiZarrToZarr(\n",
    "    path=ref_files,\n",
    "    indicts=ref_files,\n",
    "    remote_protocol=\"s3\",\n",
    "    target_options=so,\n",
    "    remote_options=so,\n",
    "    coo_map={\"time\": fn_to_time},\n",
    "    coo_dtypes={\"time\": np.dtype(\"M8[s]\")},\n",
    "    concat_dims=[\"time\"],\n",
    "    identical_dims=[\"X\", \"Y\"],\n",
    ")\n",
    "\n",
    "# save translate reference in memory for later visualization\n",
    "multi_kerchunk = mzz.translate()\n",
    "\n",
    "# Write kerchunk .json record\n",
    "output_fname = \"RADAR.json\"\n",
    "with open(f\"{output_fname}\", \"wb\") as f:\n",
    "    f.write(ujson.dumps(multi_kerchunk).encode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Combined Reference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\n",
    "    \"reference\",\n",
    "    fo=\"RADAR.json\",\n",
    "    remote_protocol=\"s3\",\n",
    "    remote_options={\"anon\": True},\n",
    "    skip_instance_cache=True,\n",
    ")\n",
    "m = fs.get_mapper(\"\")\n",
    "ds = xr.open_dataset(m, engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `xrefcoord` to Generate Coordinates\n",
    "When using `Kerchunk` to generate reference datasets for GeoTIFF's, only the dimensions are preserved. `xrefcoord` is a small utility that allows us to generate coordinates for these reference datasets using the geospatial metadata. Similar to other accessor add-on libraries for `Xarray` such as `rioxarray` and `xwrf`, `xrefcord` provides an accessor for an `Xarray` dataset. Importing `xrefcoord` allows us to use the `.xref` accessor to access additional methods. \n",
    "\n",
    "In the following cell, we will use the `generate_coords` method to build coordinates for the `Xarray` dataset. `xrefcoord` is *very experimental* and makes assumptions about the underlying data, such as each variable shares the same dimensions etc. Use with caution!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate coordinates from reference dataset\n",
    "ref_ds = ds.xref.generate_coords(time_dim_name=\"time\", x_dim_name=\"X\", y_dim_name=\"Y\")\n",
    "# Rename to rain accumulation in 24 hour period\n",
    "ref_ds = ref_ds.rename({\"0\": \"rr24h\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Map\n",
    "\n",
    "Here we are using `Xarray` to select a single time slice and create a map of 24 hour accumulated rainfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ds[\"rr24h\"].where(ref_ds.rr24h < 60000).isel(time=0).plot(robust=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Time-Series\n",
    "\n",
    "Next we are plotting accumulated rain as a function of time for a specific point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ds[\"rr24h\"][:, 700, 700].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerchunk-cookbook-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
