{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6c39c84",
   "metadata": {},
   "source": [
    "# GRIB2\n",
    "Generating Kerchunk References from GRIB2 files\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3127ae9f",
   "metadata": {},
   "source": [
    "<img src=\"../images/GRIB2.png\" width=350 alt=\"HRRR GRIB2\"></img>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9eced552",
   "metadata": {},
   "source": [
    "## Overview\n",
    "   \n",
    "Within this notebook, we will cover:\n",
    "\n",
    "1. Generating a list of GRIB2 files on a remote filesystem using `fsspec`\n",
    "1. How to create reference files of GRIB2 files using ``Kerchunk`` \n",
    "1. Combining multiple `Kerchunk` reference files using `MultiZarrToZarr`\n",
    "\n",
    "This notebook shares many similarities with the [Multi-File Datasets with Kerchunk](../foundations/kerchunk_multi_file.ipynb) and the [NetCDF/HDF5 Argentinian Weather Dataset Case Study](../case_studies/ARG_Weather.ipynb), however this case studies examines another data format and uses `kerchunk.scan_grib` to create reference files. \n",
    "\n",
    "This notebook borrows heavily from this [GIST](https://gist.github.com/peterm790/92eb1df3d58ba41d3411f8a840be2452) created by [Peter Marsh](https://gist.github.com/peterm790). \n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](../foundations/kerchunk_basics) | Required | Core |\n",
    "| [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file) | Required | Core |\n",
    "| [Kerchunk and Dask](../foundations/kerchunk_dask) | Required | Core |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Required | IO/Visualization |\n",
    "- **Time to learn**: 45 minutes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043365b",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "`Kerchunk` supports multiple input file formats. One of these is `GRIB2(GRIdded Information in Binary form)`, which is a binary file format primary used in meteorology and weather datasets. Similar to NetCDF/HDF5, GRIB2 does not support efficient, parallel access. Using `Kerchunk`, we can read this legacy format as if it were an ARCO (Analysis-Ready, Cloud-Optimized) data format such as Zarr. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469ac4",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "\n",
    "The `HRRR` is a NOAA real-time 3-km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation. Radar data is assimilated in the HRRR every 15 min over a 1-h period adding further detail to that provided by the hourly data assimilation from the 13km radar-enhanced Rapid Refresh.\n",
    "NOAA releases a copy of this dataset via the AWS Registry of Open Data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ce9e2",
   "metadata": {},
   "source": [
    "## Flags\n",
    "In the section below, set the `subset` flag to be `True` (default) or `False` depending if you want this notebook to process the full file list. If set to `True`, then a subset of the file list will be processed (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec42e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0f8f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import dask\n",
    "import fsspec\n",
    "import ujson\n",
    "from distributed import Client\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.grib2 import scan_grib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858399ce",
   "metadata": {},
   "source": [
    "## Create Input File List\n",
    "\n",
    "Here we create `fsspec` files-systems for reading remote files and writing local reference files.\n",
    "Next we are using `fsspec.glob` to retrieve a list of file paths and appending the `s3://` prefix to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fsspec filesystems for reading and writing\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "\n",
    "# retrieve list of available days in archive\n",
    "days_available = fs_read.glob(\"s3://noaa-hrrr-bdp-pds/hrrr.*\")\n",
    "\n",
    "# Read HRRR GRIB2 files from April 19, 2023\n",
    "files = fs_read.glob(\"s3://noaa-hrrr-bdp-pds/hrrr.20230419/conus/*wrfsfcf01.grib2\")\n",
    "\n",
    "# Append s3 prefix for filelist\n",
    "files = sorted([\"s3://\" + f for f in files])\n",
    "\n",
    "# If the subset_flag == True (default), the list of input files will be subset to\n",
    "# speed up the processing\n",
    "if subset_flag:\n",
    "    files = files[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec2881",
   "metadata": {},
   "source": [
    "## Start a Dask Client\n",
    "\n",
    "To parallelize the creation of our reference files, we will use `Dask`. For a detailed guide on how to use Dask and Kerchunk, see the Foundations notebook: [Kerchunk and Dask](../foundations/kerchunk_dask).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ee8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=8, silence_logs=logging.ERROR)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec53627",
   "metadata": {},
   "source": [
    "## Iterate through list of files and create `Kerchunk` indicies as `.json` reference files\n",
    "\n",
    "Each input GRIB2 file contains mutiple \"messages\", each a measure of some variable on a grid, but with grid dimensions not necessarily compatible with one-another. The filter we create in the first line selects only certain types of messages, and indicated that heightAboveGround will be a coordinate of interest.\n",
    "\n",
    "We also write a separate JSON for each of the selected message, since these are the basic component data sets (see the loop over `out`).\n",
    "\n",
    "**Note**: `scan_grib` does not require a filter and will happily create a reference file for each available grib message. However when combining the grib messages using `MultiZarrToZarr` it is necessary for the messages to share a coordinate system. Thus to make our lives easier and ensure all reference outputs from `scan_grib` share a coordinate system we pass a filter argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "afilter = {\"typeOfLevel\": \"heightAboveGround\", \"level\": [2, 10]}\n",
    "so = {\"anon\": True}\n",
    "\n",
    "# We are creating a temporary directory to store the .json reference files\n",
    "# Alternately, you could write these to cloud storage.\n",
    "td = TemporaryDirectory()\n",
    "temp_dir = td.name\n",
    "temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d45880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_name(\n",
    "    file_url, message_number\n",
    "):  # create a unique name for each reference file\n",
    "    date = file_url.split(\"/\")[3].split(\".\")[1]\n",
    "    name = file_url.split(\"/\")[5].split(\".\")[1:3]\n",
    "    return f\"{temp_dir}/{date}_{name[0]}_{name[1]}_message{message_number}.json\"\n",
    "\n",
    "\n",
    "def gen_json(file_url):\n",
    "    out = scan_grib(\n",
    "        file_url, storage_options=so, inline_threshold=100, filter=afilter\n",
    "    )  # create the reference using scan_grib\n",
    "    for i, message in enumerate(\n",
    "        out\n",
    "    ):  # scan_grib outputs a list containing one reference per grib message\n",
    "        out_file_name = make_json_name(file_url, i)  # get name\n",
    "        with open(out_file_name, \"w\") as f:\n",
    "            f.write(ujson.dumps(message))  # write to file\n",
    "\n",
    "\n",
    "# Generate Dask Delayed objects\n",
    "tasks = [dask.delayed(gen_json)(fil) for fil in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab89f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start parallel processing\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "dask.compute(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5adc55",
   "metadata": {},
   "source": [
    "### Combine `Kerchunk` reference `.json` files\n",
    "\n",
    "We know that four coordinates are identical for every one of our component datasets - they are not functions of valid_time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6422fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of reference json files\n",
    "output_files = glob.glob(f\"{temp_dir}/*.json\")\n",
    "\n",
    "# Combine individual references into single consolidated reference\n",
    "mzz = MultiZarrToZarr(\n",
    "    output_files,\n",
    "    concat_dims=[\"valid_time\"],\n",
    "    identical_dims=[\"latitude\", \"longitude\", \"heightAboveGround\", \"step\"],\n",
    ")\n",
    "multi_kerchunk = mzz.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4209d",
   "metadata": {},
   "source": [
    "### Write combined `Kerchunk` reference file to `.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Kerchunk .json record\n",
    "output_fname = \"HRRR_combined.json\"\n",
    "with open(f\"{output_fname}\", \"wb\") as f:\n",
    "    f.write(ujson.dumps(multi_kerchunk).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74baf0",
   "metadata": {},
   "source": [
    "## Shut down the Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e67132",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8afa8ad8f3d27e858f1dbdc03ccd45fac432e2a03d4a98c501e197170438b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
