{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../kerchunk.png\" width=500 alt=\"Kerchunk Logo\"></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi File Datasets with Kerchunk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is intends to build off of the [kerchunks basics notebook](./kerchunk_basics.ipynb).\n",
    "\n",
    "In this tutorial we will:\n",
    "- Create a list of input paths for a collection of NetCDF files stored on the cloud.\n",
    "- Iterate through our file input list and create kerchunk reference `.jsons` for each file.\n",
    "- Combine the reference `.jsons` into a single combined dataset reference with the rechunker class, `MultiZarrToZarr`\n",
    "- Learn how to read the combined dataset  using `xarray` and `fsspec`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](./kerchunk_basics.ipynb) | Required | Basic features |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/cartopy/cartopy.html) | Recommended | IO |\n",
    "\n",
    "- **Time to learn**: 60 minutes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import ujson\n",
    "import xarray as xr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a File Pattern from a list of  input NetCDF files\n",
    "\n",
    "Below we will create a list of input files we want kerchunk to read over. In the [Kerchunk Basics Tutorial](./kerchunk_basics.ipynb), we looked at a single file of climate downscaled data over Southern Alaska. In this example, we will build off of that work and use kerchunk to combine multiple NetCDF files of this dataset into a single virtual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fsspec filesystems for reading and writing\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "fs_write = fsspec.filesystem(\"\")\n",
    "\n",
    "# Retrieve list of available days in archive for the year 2060.\n",
    "files_paths = fs_read.glob(\"s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/*\")\n",
    "\n",
    "# Here we prepend the prefix 's3://', which points to AWS.\n",
    "file_pattern = sorted([\"s3://\" + f for f in files_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365 file paths were retrieved.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(file_pattern)} file paths were retrieved.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick check, it looks like we have a list 365 file paths, which should be a year of downscaled climte data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: If you want to look at one NetCDF files before creating the kerchunk index, try uncommenting this code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Optional piece of code to view one of the NetCDF files using Xarray as fsspec.\n",
    "\n",
    "# import s3fs\n",
    "# fs = s3fs.S3FileSystem(anon=True, default_fill_cache=False)\n",
    "# with fs.open(file_pattern[0]) as fileObj:\n",
    "#     ds = xr.open_dataset(fileObj)\n",
    "#     print(ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Kerchunk References for every file in `File_Pattern` list\n",
    "\n",
    "Now that we have a list of NetCDF files, we can use kerchunk to create reference files for each one of these. To do this, we will iterate through each file and create a reference `.json`. To speed this process up, you could use `Dask`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: To speed next section up, uncomment the next cell.  This will reduce the # of input files from 365 to 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL SPEEDUP: DEFAULT IS OFF ###\n",
    "\n",
    "file_pattern = file_pattern[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:33<03:21, 33.51s/it]"
     ]
    }
   ],
   "source": [
    "# fsspec.open args\n",
    "so = dict(mode=\"rb\", anon=True, default_fill_cache=False, default_cache_type=\"first\")\n",
    "output_dir = \"./\"\n",
    "\n",
    "# Use kerchunk's SingleHdf5ToZarr to transform netcdf to kerchunk index.\n",
    "def generate_json_reference(u, output_dir: str):\n",
    "    with fs_read.open(u, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "        fname = u.split(\"/\")[-1].strip(\".nc\")\n",
    "        outf = f\"{fname}.json\"\n",
    "        with open(outf, \"wb\") as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode())\n",
    "        return outf\n",
    "\n",
    "\n",
    "# Iterate through filelist to generate kerchunked files. Good use for dask\n",
    "output_files = []\n",
    "for fil in tqdm(file_pattern):\n",
    "    outf = generate_json_reference(fil, output_dir)\n",
    "    output_files.append(outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine .json kerchunk reference files and write a combined kerchunk index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine individual references into single consolidated reference\n",
    "mzz = MultiZarrToZarr(\n",
    "    output_files,\n",
    "    concat_dims=[\"Time\"],\n",
    "    identical_dims=[\"south_north\", \"west_east\", \"interp_levels\", \"soil_layers_stag\"],\n",
    ")\n",
    "\n",
    "\n",
    "multi_kerchunk = mzz.translate()\n",
    "\n",
    "# Write kerchunk .json record\n",
    "output_fname = \"combined_kerchunk.json\"\n",
    "with open(f\"{output_fname}\", \"wb\") as f:\n",
    "    f.write(ujson.dumps(multi_kerchunk).encode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write combined kerchunk index for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write kerchunk .json record\n",
    "output_fname = \"combined_kerchunk.json\"\n",
    "with open(f\"{output_fname}\", \"wb\") as f:\n",
    "    f.write(ujson.dumps(multi_kerchunk).encode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open combined kerchunk dataset with `fsspec` and `Xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset as zarr object using fsspec reference file system and xarray\n",
    "fs = fsspec.filesystem(\n",
    "    \"reference\", fo=multi_kerchunk, remote_protocol=\"s3\", remote_options={\"anon\": True}\n",
    ")\n",
    "m = fs.get_mapper(\"\")\n",
    "ds = xr.open_dataset(m, engine=\"zarr\", backend_kwargs=dict(consolidated=False))\n",
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a slice of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(Time=0).SNOW.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8afa8ad8f3d27e858f1dbdc03ccd45fac432e2a03d4a98c501e197170438b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
