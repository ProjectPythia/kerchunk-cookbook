{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6c39c84",
   "metadata": {},
   "source": [
    "# Kerchunk and GRIB2:  A Case Study using NOAA's High-Resolution Rapid Refresh (HRRR) Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3127ae9f",
   "metadata": {},
   "source": [
    "<img src=\"../images/GRIB2.png\" width=400 alt=\"HRRR GRIB2\"></img>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4043365b",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "`Kerchunk` supports multiple input file formats. One of these is `GRIB2(GRIdded Information in Binary form)`, which is a binary file format primary used in meteorology and weather datasets. Similar to NetCDF/HDF5, GRIB2 does not support efficient, parallel access. Using Kerchunk, we can read this legacy format as if it were an ARCO (Analysis-Ready, Cloud-Optimized) data format such as Zarr. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3469ac4",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "\n",
    "The `HRRR` is a NOAA real-time 3-km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation. Radar data is assimilated in the HRRR every 15 min over a 1-h period adding further detail to that provided by the hourly data assimilation from the 13km radar-enhanced Rapid Refresh.\n",
    "NOAA releases a copy of this dataset via the AWS Registry of Open Data. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8be0b3f2",
   "metadata": {},
   "source": [
    "## Overview\n",
    "   \n",
    "Within this notebook, we will cover:\n",
    "\n",
    "1. How to access remote GRIB2 data using `Kerchunk`\n",
    "1. Combining multiple `Kerchunk` reference files using `MultiZarrToZarr`\n",
    "1. Reading the output with `Xarray` and `Intake`\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](../foundations/kerchunk_basics) | Required | Core |\n",
    "| [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file) | Required | Core |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Required | IO/Visualization |\n",
    "| [Intake Introduction](https://projectpythia.org/intake-cookbook/notebooks/intake_introduction.html) | Recommended | IO |\n",
    "- **Time to learn**: 45 minutes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8d0f8f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706c1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import glob\n",
    "\n",
    "import fsspec\n",
    "import ujson\n",
    "import xarray as xr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.grib2 import scan_grib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "858399ce",
   "metadata": {},
   "source": [
    "## Create Input File List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fb2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fsspec filesystems for reading and writing\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "fs_write = fsspec.filesystem(\"\")\n",
    "\n",
    "# retrieve list of available days in archive\n",
    "days_available = fs_read.glob(\"s3://noaa-hrrr-bdp-pds/hrrr.*\")\n",
    "# Read HRRR GRIB2 files from latest day\n",
    "files = fs_read.glob(f\"s3://{days_available[-1]}/conus/*wrfsfcf01.grib2\")\n",
    "# Append s3 prefix for filelist\n",
    "files = sorted([\"s3://\" + f for f in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288e16cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t00z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t01z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t02z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t03z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t04z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t05z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t06z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t07z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t08z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t09z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t10z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t11z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t12z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t13z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t14z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t15z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t16z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t17z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t18z.wrfsfcf01.grib2',\n",
       " 's3://noaa-hrrr-bdp-pds/hrrr.20230119/conus/hrrr.t19z.wrfsfcf01.grib2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ec53627",
   "metadata": {},
   "source": [
    "## Iterate through filelist and create Kerchunk indicies as .json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa89197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: scan_grib does not require a filter and will happily create a reference file for each available grib message. However when combining the grib messages using MultiZarrToZarr it is neccassary for the messages to share a coordinate system. Thus to make our lives easier and ensure all reference outputs from scan_grib share a coordinate system we pass a filter argument.\n",
    "afilter = {\"typeOfLevel\": \"heightAboveGround\", \"level\": [2, 10]}\n",
    "so = {\"anon\": True}\n",
    "json_dir = \".\"\n",
    "\n",
    "\n",
    "def make_json_name(\n",
    "    file_url, message_number\n",
    "):  # create a unique name for each reference file\n",
    "    date = file_url.split(\"/\")[3].split(\".\")[1]\n",
    "    name = file_url.split(\"/\")[5].split(\".\")[1:3]\n",
    "    return f\"{json_dir}{date}_{name[0]}_{name[1]}_message{message_number}.json\"\n",
    "\n",
    "\n",
    "def gen_json(file_url):\n",
    "\n",
    "    out = scan_grib(\n",
    "        file_url, storage_options=so, inline_threshold=100, filter=afilter\n",
    "    )  # create the reference using scan_grib\n",
    "    for i, message in enumerate(\n",
    "        out\n",
    "    ):  # scan_grib outputs a list containing one reference per grib message\n",
    "        out_file_name = make_json_name(file_url, i)  # get name\n",
    "        with fs_write.open(out_file_name, \"w\") as f:\n",
    "            f.write(ujson.dumps(message))  # write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d45880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 1/20 [09:11<2:54:37, 551.44s/it]"
     ]
    }
   ],
   "source": [
    "# Iterate through available input files and generate json reference files. Note: This could optionally be parallelized.\n",
    "for f in tqdm(files):\n",
    "    gen_json(f)\n",
    "\n",
    "reference_jsons = sorted(fs_write.glob(json_dir + \"*.json\"))  # get list of file names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf5adc55",
   "metadata": {},
   "source": [
    "## Combine .json Kerchunk reference files and write a combined Kerchunk index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6422fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine individual references into single consolidated reference\n",
    "mzz = MultiZarrToZarr(\n",
    "    reference_jsons,\n",
    "    concat_dims=[\"valid_time\"],\n",
    "    identical_dims=[\"latitude\", \"longitude\", \"heightAboveGround\", \"step\"],\n",
    ")\n",
    "multi_kerchunk = mzz.translate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28283d9e",
   "metadata": {},
   "source": [
    "## Load Kerchunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05643d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset as zarr object using fsspec reference file system and xarray\n",
    "fs = fsspec.filesystem(\n",
    "    \"reference\", fo=multi_kerchunk, remote_protocol=\"s3\", remote_options={\"anon\": True}\n",
    ")\n",
    "m = fs.get_mapper(\"\")\n",
    "ds = xr.open_dataset(\n",
    "    m, engine=\"zarr\", backend_kwargs=dict(consolidated=False), chunks={\"valid_time\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"2t\"][-1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"2t\"][:, 500, 500].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8afa8ad8f3d27e858f1dbdc03ccd45fac432e2a03d4a98c501e197170438b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
