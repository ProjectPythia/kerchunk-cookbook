{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1aec28e",
   "metadata": {},
   "source": [
    "# Kerchunk and NetCDF/HDF5:  A Case Study using the National Water Model - Short Range Forecast Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8f2fdcf",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "NetCDF/HDF5 is one of the most universally adopted file formats in earth sciences, with support of much of the community as well as scientific agencies, data centers and university labs. A huge amount of legacy data has been generated in this format. Fortunately, using `kerchunk`, we can read these datasets as if they were Zarr."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fecb6ea",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "The National Water Model dataset is a produced by the National Oceanic and Atmospheric Administations (NOAA's) Office of Water Prediction. It is a forecast model of water resources, providing multiple variables across the continental United States (CONUS). \n",
    "This dataset is available via the Registry of Open Data on AWS as a collection of netCDF files that do not require any login authentication. Using `kerchunk`, we will demonstrate how to build a kerchunk index so that this dataset can be read as if it were an ARCO (Analysis-Ready, Cloud-Optimized) dataset. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f91dc53",
   "metadata": {},
   "source": [
    "## Overview\n",
    "   \n",
    "Within this notebook, we will cover:\n",
    "\n",
    "1. How to access remote NetCDF data using kerchunk\n",
    "1. Combining multiple kerchunk reference files using `MultiZarrToZarr`\n",
    "1. Reading the output with xarray and intake\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](../foundations/kerchunk_basics) | Required | Core |\n",
    "| [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file) | Required | Core |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Required | IO/Visualization |\n",
    "| [Intake Introduction](https://projectpythia.org/intake-cookbook/notebooks/intake_introduction.html) | Recommended | IO |\n",
    "- **Time to learn**: 45 minutes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85ce1f3e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca60eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Imports\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "import fsspec_reference_maker\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import os\n",
    "import ujson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a9fbd77",
   "metadata": {},
   "source": [
    "## Create Input File List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdeafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fsspec filesystem for AWS s3.\n",
    "fs = fsspec.filesystem('s3', anon=True, skip_instance_cache=True)\n",
    "# Use fsspec and glob to retrieve a list of all netCDF files to be used in the kerchunk index generation.\n",
    "flist = fs.glob(f'noaa-nwm-pds/nwm.*/short_range/nwm.*.short_range.channel_rt.f001.conus.nc')\n",
    "\n",
    "\n",
    "last_dir = f'{os.path.dirname(flist[-1])}'\n",
    "last_file = os.path.basename(flist[-1]).split('.')\n",
    "last_files = fs.glob(f'{last_dir}/{last_file[0]}.{last_file[1]}.{last_file[2]}.channel_rt.*.conus.nc')\n",
    "\n",
    "# Skip the first of the last_files since it's a duplicate\n",
    "flist.extend(last_files[1:])\n",
    "\n",
    "# We need to include the \"s3://\" prefix to the list of files\n",
    "# so that fsspec will recognize that these JSON files are on S3. There is no \"storage_\n",
    "urls = [\"s3://\" + f for f in flist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963edaee",
   "metadata": {},
   "source": [
    "## Iterate through filelist and create kerchunk indicies as .json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798df6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fsspec.open args\n",
    "so = dict(mode='rb', anon=True, default_fill_cache=False, default_cache_type='first')\n",
    "output_dir = './NWM_dir'\n",
    "\n",
    "# Use kerchunk's SingleHdf5ToZarr to transform netcdf to kerchunk index.\n",
    "def gen_json(u, output_dir: str):\n",
    "    with fs.open(u, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "        p = u.split('/')\n",
    "        date = p[3]\n",
    "        fname = p[5]\n",
    "        outf = f'{output_dir}/{date}.{fname}.json'\n",
    "        with open(outf, 'wb') as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode());\n",
    "        return outf\n",
    "\n",
    "# Iterate through filelist to generate kerchunked files. Good use for dask\n",
    "output_files = []\n",
    "for fil in tqdm(urls):\n",
    "    outf = gen_json(fil, output_dir)\n",
    "    output_files.append(outf)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758c452",
   "metadata": {},
   "source": [
    "## Combine .json kerchunk reference files and write a combined kerchunk index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f37ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine single kerchunk output reference files into a multi file kerchunk dataset\n",
    "mzz = MultiZarrToZarr(output_files, concat_dims=['time'])\n",
    "d = mzz.translate()\n",
    "\n",
    "# Write kerchunk .json record\n",
    "output_fname = 'NWM.json'\n",
    "with open(f'{output_fname}', 'wb') as f:\n",
    "    f.write(ujson.dumps(d).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e27f3",
   "metadata": {},
   "source": [
    "## Load kerchunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fsspec reference filesystem from the kerchunk output\n",
    "fs = fsspec.filesystem(\"reference\", fo=output_fname)\n",
    "m = fs.get_mapper(\"\")\n",
    "ds = xr.open_zarr(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, Jul 19 2021, 09:37:30) \n[Clang 13.0.0 (clang-1300.0.27.3)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
