{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1aec28e",
   "metadata": {},
   "source": [
    "# Kerchunk and NetCDF/HDF5:  A Case Study using the National Water Model - Short Range Forecast Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8f2fdcf",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "NetCDF/HDF5 is one of the most universally adopted file formats in earth sciences, with support of much of the community as well as scientific agencies, data centers and university labs. A huge amount of legacy data has been generated in this format. Fortunately, using `Kerchunk`, we can read these datasets as if they were Zarr."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fecb6ea",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "The National Water Model dataset is a produced by the National Oceanic and Atmospheric Administations (NOAA's) Office of Water Prediction. It is a forecast model of water resources, providing multiple variables across the continental United States (CONUS). \n",
    "This dataset is available via the Registry of Open Data on AWS as a collection of netCDF files that do not require any login authentication. Using `Kerchunk`, we will demonstrate how to build a `Kerchunk` index so that this dataset can be read as if it were an ARCO (Analysis-Ready, Cloud-Optimized) dataset. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f91dc53",
   "metadata": {},
   "source": [
    "## Overview\n",
    "   \n",
    "Within this notebook, we will cover:\n",
    "\n",
    "1. How to access remote NetCDF data using `Kerchunk`\n",
    "1. Combining multiple `Kerchunk` reference files using `MultiZarrToZarr`\n",
    "1. Reading the output with `Xarray` and `Intake`\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](../foundations/kerchunk_basics) | Required | Core |\n",
    "| [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file) | Required | Core |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Required | IO/Visualization |\n",
    "| [Intake Introduction](https://projectpythia.org/intake-cookbook/notebooks/intake_introduction.html) | Recommended | IO |\n",
    "- **Time to learn**: 45 minutes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85ce1f3e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca60eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Imports\n",
    "import os\n",
    "\n",
    "import fsspec\n",
    "import fsspec_reference_maker\n",
    "import ujson\n",
    "import xarray as xr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a9fbd77",
   "metadata": {},
   "source": [
    "## Create Input File List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cdeafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an `fsspec` filesystem for AWS s3.\n",
    "fs = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "\n",
    "# Use fsspec and glob to retrieve a list of all netCDF files to be used in the kerchunk index generation.\n",
    "flist = fs.glob(\n",
    "    f\"noaa-nwm-pds/nwm.*/short_range/nwm.*.short_range.channel_rt.f001.conus.nc\"\n",
    ")\n",
    "\n",
    "\n",
    "last_dir = f\"{os.path.dirname(flist[-1])}\"\n",
    "last_file = os.path.basename(flist[-1]).split(\".\")\n",
    "last_files = fs.glob(\n",
    "    f\"{last_dir}/{last_file[0]}.{last_file[1]}.{last_file[2]}.channel_rt.*.conus.nc\"\n",
    ")\n",
    "\n",
    "# Skip the first of the last_files since it's a duplicate\n",
    "flist.extend(last_files[1:])\n",
    "\n",
    "# We need to include the \"s3://\" prefix to the list of files\n",
    "# so that fsspec will recognize that these JSON files are on S3. There is no \"storage_\n",
    "urls = [\"s3://\" + f for f in flist]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "963edaee",
   "metadata": {},
   "source": [
    "## Iterate through `flist` and create `Kerchunk` indicies as `.json` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798df6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 25/733 [03:38<1:32:16,  7.82s/it]"
     ]
    }
   ],
   "source": [
    "# fsspec.open args\n",
    "so = dict(mode=\"rb\", anon=True, default_fill_cache=False, default_cache_type=\"first\")\n",
    "output_dir = \"./\"\n",
    "\n",
    "# Use Kerchunk's `SingleHdf5ToZarr` method to create a `Kerchunk` index from a NetCDF file.\n",
    "def gen_json(u, output_dir: str):\n",
    "    with fs.open(u, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "        p = u.split(\"/\")\n",
    "        date = p[3]\n",
    "        fname = p[5]\n",
    "        outf = f\"{output_dir}/{date}.{fname}.json\"\n",
    "        with open(outf, \"wb\") as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode())\n",
    "        return outf\n",
    "\n",
    "\n",
    "# Iterate through filelist to generate Kerchunked files. Good use for `Dask`\n",
    "output_files = []\n",
    "for fil in tqdm(urls):\n",
    "    outf = gen_json(fil, output_dir)\n",
    "    output_files.append(outf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6758c452",
   "metadata": {},
   "source": [
    "## Combine .json `Kerchunk` reference files and write a combined `Kerchunk` index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f37ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine single `Kerchunk`` output reference files into a multi-file `Kerchunk`` dataset\n",
    "mzz = MultiZarrToZarr(output_files, concat_dims=[\"time\"])\n",
    "d = mzz.translate()\n",
    "\n",
    "# Write `Kerchunk` `.json` record\n",
    "output_fname = \"NWM.json\"\n",
    "with open(f\"{output_fname}\", \"wb\") as f:\n",
    "    f.write(ujson.dumps(d).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e27f3",
   "metadata": {},
   "source": [
    "## Load kerchunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an `fsspec`` reference filesystem from the `Kerchunk`` output\n",
    "fs = fsspec.filesystem(\"reference\", fo=output_fname)\n",
    "m = fs.get_mapper(\"\")\n",
    "ds = xr.open_zarr(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8afa8ad8f3d27e858f1dbdc03ccd45fac432e2a03d4a98c501e197170438b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
