{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../images/dask.png\" width=200 alt=\"Dask Logo\"></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel virtual dataset creation with VirtualiZarr, Kerchunk, and Dask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "   \n",
    "In this notebook we will cover:\n",
    "\n",
    "1. How to parallelize the creation of virtual datasets using the `Dask` library.\n",
    "\n",
    "This notebook builds upon the [Basics of virtual Zarr stores](./01_kerchunk_basics.ipynb) and the [Multi-file virtual datasets with VirtualiZarr](./02_kerchunk_multi_file.ipynb) notebooks. A basic understanding of `Dask` will be helpful, but is not required. This notebook is not intended as a tutorial for using `Dask`, but will show how to use `Dask` to greatly speedup the the generation of virtual datasets.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Basics of virtual Zarr stores](./01_kerchunk_basics.ipynb) | Required | Core |\n",
    "| [Multi-file virtual datasets with VirtualiZarr](./02_kerchunk_multi_file.ipynb) | Required | Core |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Recommended | IO/Visualization |\n",
    "| [Intro to Dask](https://tutorial.dask.org/00_overview.html) | Recommended | Parallel Processing |\n",
    "\n",
    "- **Time to learn**: 45 minutes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask and Parallel Processing\n",
    "\n",
    "Dask is a `Python` library for parallel computing. It plays well with `Xarray`, but can be used in many ways across the `Python` ecosystem. This notebook is not intended to be a guide for how to use `Dask`, but just an example of how to use `Dask` to parallelize some `VirtualiZarr` and `Kerchunk` functionality. \n",
    "\n",
    "In the previous notebook [Multi-file virtual datasets with VirtualiZarr](./02_kerchunk_multi_file.ipynb), we were looking at daily downscaled climate data over South-Eastern Alaska. We created a virtual dataset for each input file using list comprehension in Python.\n",
    "\n",
    "With `Dask`, we can call `open_virtual_dataset` in parallel, which allows us to create multiple virtual datasets at the same time.\n",
    "\n",
    "Further on in this notebook, we will show how using `Dask` can greatly speed-up the process of creating a virtual datasets.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the `Dask` Client\n",
    "\n",
    "In the code below, we are importing `Dask Disributed` and creating a `client`. This is the start of our parallel `Kerchunk` data processing. We are passing the argument `n_workers=8`. This will inform the `Dask` client on some of the resource limitations. \n",
    "\n",
    "Note: Depending on if you are running on a small machine such as a laptop or a larger compute hub, these resources could be tuned to improve performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from distributed import Client\n",
    "\n",
    "client = Client(n_workers=8, silence_logs=logging.ERROR)\n",
    "client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binder Specific Setup\n",
    "\n",
    "If you are running this tutorial on `Binder`, the configuration may look slightly different. \n",
    "\n",
    "Once you start the client, some information should be returned to you as well as a button that says:\n",
    "\n",
    "**Launch Dashboard in JupyterLab**\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/client_binder.png\" width=\"600\"/> \n",
    "  \n",
    "\n",
    "Once you click that button, multiple windows of the `Dask` dashboard should open up. \n",
    "\n",
    "\n",
    "<img src=\"../images/binder_client2.png\" width=\"600\"/> \n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building off of our Previous Work\n",
    "In the next section, we will re-use some of the code from the [multi-file virtual datasets with VirtualiZarr](./02_kerchunk_multi_file.ipynb) notebook. However, we will modify it slightly to make it compatible with `Dask`.\n",
    "\n",
    "The following two cells should look the same as before.  As a reminder we are importing the required libraries, using `fsspec` to create a list of our input files and setting up some kwargs for `fsspec` to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import fsspec\n",
    "from virtualizarr import open_virtual_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fsspec filesystems for reading and writing\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "\n",
    "# Retrieve list of available days in archive for the year 2060.\n",
    "files_paths = fs_read.glob(\"s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/*\")\n",
    "\n",
    "# Here we prepend the prefix 's3://', which points to AWS.\n",
    "files_paths = sorted([\"s3://\" + f for f in files_paths])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset the Data\n",
    "To speed up our example, lets take a subset of the year of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the subset_flag == True (default), the list of input files will\n",
    "# be subset to speed up the processing\n",
    "subset_flag = True\n",
    "if subset_flag:\n",
    "    files_paths = files_paths[0:4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Specific Changes\n",
    "\n",
    "Here is the section of code that will change. Instead of iterating through each input file and using `open_virtual_dataset` to create the virtual datasets, we are iterating through our input file list and creating `Dask Delayed Objects`. It is not super important to understand this, but a `Dask Delayed Object` is lazy, meaning it is not computed eagerly. Once we have iterated through all our input files, we end up with a list of `Dask Delayed Objects`. \n",
    "\n",
    "When we are ready, we can call `dask.compute` on this list of delayed objects to create virtual datasets in parallel. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_virtual_dataset(file, storage_options):\n",
    "    return open_virtual_dataset(\n",
    "        file, indexes={}, reader_options={\"storage_options\": storage_options}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_options = dict(anon=True, default_fill_cache=False, default_cache_type=\"none\")\n",
    "# Generate Dask Delayed objects\n",
    "tasks = [\n",
    "    dask.delayed(generate_virtual_dataset)(file, storage_options)\n",
    "    for file in files_paths\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Task Graph\n",
    "\n",
    "Once you call `dask.compute` it can be hard to understand what is happening and how far along the process is at any time. Fortunately, `Dask` has a built in dashboard to help visualize your progress.\n",
    "\n",
    "### Running this notebook locally\n",
    "When you first initialized the `Dask` `client` earlier on, it should have returned some information including an address to the `dashboard`. For example: `http://127.0.0.1:8787/status`\n",
    "\n",
    "\n",
    "By navigating to this address, you should a `Dask` dashboard that looks something like this.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"../images/empty_dashboard.png\" width=\"700\"/> \n",
    "  \n",
    "<br/><br/>\n",
    "\n",
    "When you call `dask.compute(tasks)`, the dashboard should populate with a bunch of tasks. In the dashboard you can monitor your progress, see how resources are being used as well as well as countless other functionality. \n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"../images/dashboard.png\"  width=\"700\"/> \n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "### Running on Binder\n",
    "\n",
    "If you are running this example notebook on `Binder`, the `Dask` dashboard should look slightly different. Since `Binder` is running the notebook on another computer, navigating to localhost will give you an error. \n",
    "The `Binder` specific `Dask` graph should look something more like this:\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"../images/task_stream_full.png\"  width=\"700\"/> \n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Dask Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut down the Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing\n",
    "\n",
    "To demonstrate how Dask can speed-up your virtual dataset generation, the next section will show the timing of generating reference files with and without Dask.\n",
    "For reference, the timing was run on a large AWS Jupyter-Hub (managed by the fine folks at [2i2c](https://2i2c.org/)) with ~16 CPU and ~64 GB RAM. It is also important to note that the data is also hosted on AWS. \n",
    "\n",
    "\n",
    "\n",
    "| Serial Virtualization      | Parallel Virtualization (Dask) |\n",
    "| -------------------- | ------------------------ |\n",
    "| 7 min 22 s           | 36 s                   |\n",
    "\n",
    "\n",
    "\n",
    "Running our `Dask` version on the year of data took only ~36 seconds. In comparison, creating the `VirtualiZarr` virtual datasets one-by-one took about 7 minutes and 22 seconds.\n",
    "\n",
    "\n",
    "Just by changing a few lines of code and using `Dask`, we got our code to run **11x faster**. One other detail to note is that there is usually a bit of a delay as `Dask` builds its task graph before any of the tasks are started. All that to say, you may see even better performance when using `Dask`, `VirtualiZarr`, and `Kerchunk` on larger datasets.\n",
    "\n",
    "Note: These timings may vary for you. There are many factors that may affect performance, such as:\n",
    "- Geographical location of your compute and the source data\n",
    "- Internet speed\n",
    "- Compute resources, IO limits and # of workers given to `Dask`\n",
    "- Location of where to write reference files to (cloud vs local)\n",
    "\n",
    "This is meant to be an example of how `Dask` can be used to speed-up `Kerchunk` not a detailed benchmark in `Kerchunk/Dask` performance. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this notebook we demonstrated how `Dask` can be used to parallelize the creation of virtual datasets. In the following `Case Studies` section, we will walk though examples of using `VirtualiZarr` and `Kerchunk` with `Dask` to create virtual cloud-optimized datasets. \n",
    "\n",
    "Additionally, if you wish to explore more of `Kerchunk's` `Dask` integration, you can try checking out `Kerchunk's` [auto_dask](https://fsspec.github.io/kerchunk/reference.html?highlight=auto%20dask#kerchunk.combine.auto_dask) method, which combines many of the `Dask` setup steps into a single convenient function. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerchunk-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
