{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../thumbnail.png\" width=500 alt=\"Kerchunk Logo\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerchunk Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is intended as an introduction to using `Kerchunk`.\n",
    "In this tutorial we will:\n",
    "- Scan a single NetCDF4/HDF5 file to create a `Kerchunk` virtual dataset\n",
    "- Learn how to use the output  using `Xarray` and `fsspec`.\n",
    "\n",
    "While this notebook only examines using `Kerchunk` on a single NetCDF file, `Kerchunk` can be used to create virtual `Zarr` datasets from collections of many input files. In the following notebook, we will demonstrate this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Helpful | Basic features |\n",
    "\n",
    "- **Time to learn**: 60 minutes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Here we will import a few `Python` libraries to help with our data processing. \n",
    "- `fsspec` will be used to read remote and local filesystems. \n",
    "- `kerchunk.hdf` will be used to read a NetCDF file and create a `Kerchunk` reference set.\n",
    "- `ujson` for writing the `Kerchunk` output to the `.json` file format.\n",
    "- `Xarray` for examining out output dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import kerchunk.hdf\n",
    "import ujson\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define kwargs for `fsspec`\n",
    "\n",
    "In the dictionary definition in the next cell, we are passing options to [`fsspec.open`](https://filesystem-spec.readthedocs.io/en/latest/api.html?highlight=.open#fsspec.open). Any additional kwargs passed in this dictionary through `fsspec.open` will pass as kwargs to the file system, in our case `s3`. The API docs for the `s3fs` filesystem spec can be found [here](https://s3fs.readthedocs.io/en/latest/api.html).\n",
    "\n",
    "In this example we are passing a few kwargs. In short they are:\n",
    "- `anon=True`: This is a `s3fs` kwarg that specifies you are not passing any connection credentials and are connecting to a public bucket.\n",
    "- `default_fill_cache=False`: `s3fs` kwarg that avoids caching in between chunks of files. This may lower memory usage when reading large files.\n",
    "- `default_cache_type=\"first\"`: `fsspec` kwarg that specifies the caching strategy used by `fsspec`. In this case, `first` caches the first block of a file only.\n",
    "\n",
    "Don't worry too much about the details here; the cache options are those that have typically proven efficient for HDF5 files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so = dict(anon=True, default_fill_cache=False, default_cache_type=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse a single NetCDF file with kerchunk\n",
    "\n",
    "Below we will access a NetCDF file stored on the AWS cloud. This dataset is a single time slice of a climate downscaled product for Alaska.\n",
    "\n",
    "The steps in the cell below are as follows:\n",
    "1. Define the url that points to the `NetCDF` file we want to process\n",
    "1. Use `fsspec.open` along with the dictionary of arguments we created (`so`) to open the URL pointing to the NetCDF file.\n",
    "1. Use `kerchunk.hdf.SingleHdf5ToZarr` method to read through the `NetCDF` file and extract the byte ranges, compression information and metadata.\n",
    "1. Use `Kerchunk's` `.translate` method on the output from the `kerchunk.hdf.SingleHdf5ToZarr` to translate content of the NetCDF file into the `Zarr` format.\n",
    "1. Create a `.json` file named `single_file_kerchunk.json` and write the dataset information to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input URL to dataset. Note this is a netcdf file stored on s3 (cloud dataset).\n",
    "url = \"s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/WRFDS_2060-01-01.nc\"\n",
    "\n",
    "# Uses kerchunk to scan through the netcdf file to create kerchunk mapping and\n",
    "# then save output as .json.\n",
    "# Note: In this example, we write the kerchunk output to a .json file.\n",
    "# You could also keep this information in memory and pass it to fsspec\n",
    "with fsspec.open(url, **so) as inf:\n",
    "    h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, url, inline_threshold=100)\n",
    "    h5chunks.translate()\n",
    "    with open(\"single_file_kerchunk.json\", \"wb\") as f:\n",
    "        f.write(ujson.dumps(h5chunks.translate()).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `Kerchunk` Reference File\n",
    "\n",
    "In the section below we will use `fsspec.filesystem` along with the `Kerchunk` `.json` reference file to open the `NetCDF` file as if it were a `Zarr` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fsspec to create filesystem from .json reference file\n",
    "fs = fsspec.filesystem(\n",
    "    \"reference\",\n",
    "    fo=\"single_file_kerchunk.json\",\n",
    "    remote_protocol=\"s3\",\n",
    "    remote_options=dict(anon=True),\n",
    "    skip_instance_cache=True,\n",
    ")\n",
    "\n",
    "# load kerchunked dataset with xarray\n",
    "ds = xr.open_dataset(\n",
    "    fs.get_mapper(\"\"), engine=\"zarr\", backend_kwargs={\"consolidated\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.TMAX.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the original .nc file size here is 16.8MB, and the created JSON is 26.5kB. These files also tend to compress very well. As you can see, it the JSON can be written anywhere, and gives us access to the underlying data, reading only the chunks we need from remote without downloading the whole file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerchunk-cookbook-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "89095a95fbc59e1db286735bee0073a08e46abd63daa66f53634eb5c8cc2192a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
