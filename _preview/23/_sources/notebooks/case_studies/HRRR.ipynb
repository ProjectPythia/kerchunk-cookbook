{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c39c84",
   "metadata": {},
   "source": [
    "# Kerchunk and GRIB2:  A Case Study using NOAA's High-Resolution Rapid Refresh (HRRR) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127ae9f",
   "metadata": {},
   "source": [
    "<img src=\"../images/GRIB2.png\" width=400 alt=\"HRRR GRIB2\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eced552",
   "metadata": {},
   "source": [
    "## Overview\n",
    "   \n",
    "Within this notebook, we will cover:\n",
    "\n",
    "1. Generating a list of GRIB2 files on a remote filesystem using `fsspec`\n",
    "1. How to create reference files of GRIB2 files using ``Kerchunk`` \n",
    "1. Combining multiple `Kerchunk` reference files using `MultiZarrToZarr`\n",
    "1. Reading the output with `Xarray` and `Intake`\n",
    "\n",
    "This notebook shares many similarities with the [Multi-File Datasets with Kerchunk](../foundations/kerchunk_multi_file.ipynb) and the [NetCDF/HDF5 National Water Model Case Study](../case_studies/NWM.ipynb), however this case studies examines another data format and uses `kerchunk.scan_grib` to create reference files. \n",
    "\n",
    "This notebook borrows heavily from this [GIST](https://gist.github.com/peterm790/92eb1df3d58ba41d3411f8a840be2452) created by [Peter Marsh](https://gist.github.com/peterm790). \n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](../foundations/kerchunk_basics) | Required | Core |\n",
    "| [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file) | Required | Core |\n",
    "| [Kerchunk and Dask](../foundations/kerchunk_dask) | Required | Core |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Required | IO/Visualization |\n",
    "| [Intake Introduction](https://projectpythia.org/intake-cookbook/notebooks/intake_introduction.html) | Recommended | IO |\n",
    "- **Time to learn**: 45 minutes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043365b",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "`Kerchunk` supports multiple input file formats. One of these is `GRIB2(GRIdded Information in Binary form)`, which is a binary file format primary used in meteorology and weather datasets. Similar to NetCDF/HDF5, GRIB2 does not support efficient, parallel access. Using `Kerchunk`, we can read this legacy format as if it were an ARCO (Analysis-Ready, Cloud-Optimized) data format such as Zarr. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469ac4",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "\n",
    "The `HRRR` is a NOAA real-time 3-km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation. Radar data is assimilated in the HRRR every 15 min over a 1-h period adding further detail to that provided by the hourly data assimilation from the 13km radar-enhanced Rapid Refresh.\n",
    "NOAA releases a copy of this dataset via the AWS Registry of Open Data. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e7ce9e2",
   "metadata": {},
   "source": [
    "## Flags\n",
    "In the section below, set the `subset` flag to be `True` (default) or `False` depending if you want this notebook to process the full file list. If set to `True`, then a subset of the file list will be processed (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec42e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0f8f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import glob\n",
    "import logging\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import dask\n",
    "import fsspec\n",
    "import ujson\n",
    "import xarray as xr\n",
    "from distributed import Client\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.grib2 import scan_grib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858399ce",
   "metadata": {},
   "source": [
    "## Create Input File List\n",
    "\n",
    "Here we create `fsspec` files-systems for reading remote files and writing local reference files.\n",
    "Next we are using `fsspec.glob` to retrieve a list of file paths and appending the `s3://` prefix to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fsspec filesystems for reading and writing\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "\n",
    "# retrieve list of available days in archive\n",
    "days_available = fs_read.glob(\"s3://noaa-hrrr-bdp-pds/hrrr.*\")\n",
    "\n",
    "# Read HRRR GRIB2 files from latest day\n",
    "files = fs_read.glob(f\"s3://{days_available[-1]}/conus/*wrfsfcf01.grib2\")\n",
    "\n",
    "# Append s3 prefix for filelist\n",
    "files = sorted([\"s3://\" + f for f in files])\n",
    "\n",
    "# If the subset_flag == True (default), the list of input files will be subset to speed up the processing\n",
    "if subset_flag:\n",
    "    files = files[0:8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bec2881",
   "metadata": {},
   "source": [
    "## Start a Dask Client\n",
    "\n",
    "To parallelize the creation of our reference files, we will use `Dask`. For a detailed guide on how to use Dask and Kerchunk, see the Foundations notebook: [Kerchunk and Dask](../foundations/kerchunk_dask).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ee8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=8, silence_logs=logging.ERROR)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec53627",
   "metadata": {},
   "source": [
    "## Iterate through list of files and create `Kerchunk` indicies as `.json` reference files\n",
    "\n",
    "Each input GRIB2 file contains mutiple \"messages\", each a measure of some variable on a grid, but with grid dimensions not necessarily compatible with one-another. The filter we create in the first line selects only certain types of messages, and indicated that heightAboveGround will be a coordinate of interest.\n",
    "\n",
    "We also write a separate JSON for each of the selected message, since these are the basic component data sets (see the loop over `out`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: scan_grib does not require a filter and will happily create a reference file for each available grib message. However when combining the grib messages using MultiZarrToZarr it is neccassary for the messages to share a coordinate system. Thus to make our lives easier and ensure all reference outputs from scan_grib share a coordinate system we pass a filter argument.\n",
    "afilter = {\"typeOfLevel\": \"heightAboveGround\", \"level\": [2, 10]}\n",
    "so = {\"anon\": True}\n",
    "\n",
    "# We are creating a temporary directory to store the .json reference files\n",
    "# Alternately, you could write these to cloud storage.\n",
    "td = TemporaryDirectory()\n",
    "temp_dir = td.name\n",
    "temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d45880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_name(\n",
    "    file_url, message_number\n",
    "):  # create a unique name for each reference file\n",
    "    date = file_url.split(\"/\")[3].split(\".\")[1]\n",
    "    name = file_url.split(\"/\")[5].split(\".\")[1:3]\n",
    "    return f\"{temp_dir}/{date}_{name[0]}_{name[1]}_message{message_number}.json\"\n",
    "\n",
    "\n",
    "def gen_json(file_url):\n",
    "    out = scan_grib(\n",
    "        file_url, storage_options=so, inline_threshold=100, filter=afilter\n",
    "    )  # create the reference using scan_grib\n",
    "    for i, message in enumerate(\n",
    "        out\n",
    "    ):  # scan_grib outputs a list containing one reference per grib message\n",
    "        out_file_name = make_json_name(file_url, i)  # get name\n",
    "        with open(out_file_name, \"w\") as f:\n",
    "            f.write(ujson.dumps(message))  # write to file\n",
    "\n",
    "\n",
    "# Generate Dask Delayed objects\n",
    "tasks = [dask.delayed(gen_json)(fil) for fil in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab89f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start parallel processing\n",
    "dask.compute(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5adc55",
   "metadata": {},
   "source": [
    "### Combine `Kerchunk` reference `.json` files\n",
    "\n",
    "We know that four coordinates are identical for every one of our component datasets - they are not functions of valid_time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6422fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of reference json files\n",
    "output_files = glob.glob(f\"{temp_dir}/*.json\")\n",
    "\n",
    "# Combine individual references into single consolidated reference\n",
    "mzz = MultiZarrToZarr(\n",
    "    output_files,\n",
    "    concat_dims=[\"valid_time\"],\n",
    "    identical_dims=[\"latitude\", \"longitude\", \"heightAboveGround\", \"step\"],\n",
    ")\n",
    "multi_kerchunk = mzz.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4209d",
   "metadata": {},
   "source": [
    "### Write combined `Kerchunk` reference file to `.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Kerchunk .json record\n",
    "output_fname = \"HRRR_combined.json\"\n",
    "with open(f\"{output_fname}\", \"wb\") as f:\n",
    "    f.write(ujson.dumps(multi_kerchunk).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28283d9e",
   "metadata": {},
   "source": [
    "## Load Kerchunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05643d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset as zarr object using fsspec reference file system and Xarray\n",
    "fs = fsspec.filesystem(\n",
    "    \"reference\", fo=multi_kerchunk, remote_protocol=\"s3\", remote_options={\"anon\": True}\n",
    ")\n",
    "m = fs.get_mapper(\"\")\n",
    "ds = xr.open_dataset(\n",
    "    m, engine=\"zarr\", backend_kwargs=dict(consolidated=False), chunks={\"valid_time\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cde8c5",
   "metadata": {},
   "source": [
    "## Plot a slice of the dataset\n",
    "\n",
    "Here we are using `Xarray` to select a single time slice of the dataset and plot a temperature map of CONUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"t2m\"][-1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"t2m\"][:, 500, 500].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be917a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8afa8ad8f3d27e858f1dbdc03ccd45fac432e2a03d4a98c501e197170438b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
