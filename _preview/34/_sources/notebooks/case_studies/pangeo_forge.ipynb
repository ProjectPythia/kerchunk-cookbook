{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerchunk and Pangeo-Forge\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial we are going to use Kerchunk to create reference files of a dataset. \n",
    "This allows us to read an entire dataset as if it were a single Zarr store instead of a collection of NetCDF files. \n",
    "Using Kerchunk, we don't have to create a copy of the data, instead we create a collection of reference files, so that the original data files can be read as if they were Zarr.\n",
    "\n",
    "\n",
    "This notebook shares some similarities with the [Multi-File Datasets with Kerchunk](../case_studies/ARG_Weather.ipynb), as they both create references from NetCDF files. However, this notebook differs as it uses `Pangeo-Forge` as the runner to create the reference files.\n",
    "\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Kerchunk Basics](../foundations/kerchunk_basics) | Required | Core |\n",
    "| [Multiple Files and Kerchunk](../foundations/kerchunk_multi_file) | Required | Core |\n",
    "| [Kerchunk and Dask](../foundations/kerchunk_dask) | Required | Core |\n",
    "| [Multi-File Datasets with Kerchunk](../case_studies/ARG_Weather.ipynb) | Required | IO/Visualization |\n",
    "\n",
    "- **Time to learn**: 45 minutes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Kerchunk\n",
    "\n",
    "For many traditional data processing pipelines, the start involves download a large amount of files to a local computer and then subsetting them for future analysis. Kerchunk gives us two large advantages: \n",
    "1. A massive reduction in used disk space.\n",
    "2. Performance improvements with through parallel, chunk-specific access of the dataset. \n",
    "\n",
    "In addition to these speedups, once the consolidated Kerchunk reference file has been created, it can be easily shared for other users to access the dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pangeo-Forge & Kerchunk\n",
    "\n",
    "Pangeo-Forge is a community project to build reproducible cloud-native ARCO (Analysis-Ready-Cloud-Optimized) datasets. The Python library (`pangeo-forge-recipes`) is the ETL pipeline to process these datasets or \"recipes\". While a majority of the recipes convert a legacy format such as NetCDF to Zarr stores, `pangeo-forge-recipes` can also use Kerchunk under the hood to create reference recipes. \n",
    "\n",
    "It is important to note that `Kerchunk` can be used independently of `pangeo-forge-recipes` and in this example, `pangeo-forge-recipes` is acting as the runner for `Kerchunk`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Pangeo-Forge & Kerchunk\n",
    "\n",
    "While you can use `Kerchunk` without `pangeo-forge`, we hope that `pangeo-forge` can be another tool to create sharable ARCO datasets using `Kerchunk`. \n",
    "A few potential benefits of creating `Kerchunk` based reference recipes with `pangeo-forge` may include:\n",
    "- Recipe processing pipelines may be more standardized than case-by-case custom Kerchunk processing functions.\n",
    "- Recipe processing can be scaled through `pangeo-forge-cloud` for large datasets.\n",
    "- The infrastructure of `pangeo-forge` in GitHub may allow more community feedback on recipes.\n",
    "- Additional features such as appending to datasets as new data is generated may be available in future releases of `pangeo-forge`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to Know The Data\n",
    "\n",
    "`gridMET` is a high-resolution daily meteorological dataset covering CONUS from 1979-2023. It is produced by the Climatology Lab at UC Merced. In this example, we are going to look create a virtual Zarr dataset of a derived variable, Burn Index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine a Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ds = xr.open_dataset(\n",
    "    \"http://thredds.northwestknowledge.net:8080/thredds/dodsC/MET/bi/bi_2021.nc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.sel(day=\"2021-08-01\").burning_index_g.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a File Pattern\n",
    "\n",
    "To build our `pangeo-forge` pipeline, we need to create a `FilePattern` object, which is composed of all of our input urls. This dataset ranges from 1979 through 2023 and is composed of one year per file. \n",
    " \n",
    "To speed up our example, we will `prune` our recipe to select the first two entries in the `FilePattern`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pangeo_forge_recipes.patterns import ConcatDim, FilePattern, MergeDim\n",
    "\n",
    "years = list(range(1979, 2022 + 1))\n",
    "\n",
    "\n",
    "time_dim = ConcatDim(\"time\", keys=years)\n",
    "\n",
    "\n",
    "def format_function(time):\n",
    "    return f\"http://www.northwestknowledge.net/metdata/data/bi_{time}.nc\"\n",
    "\n",
    "\n",
    "pattern = FilePattern(format_function, time_dim, file_type=\"netcdf4\")\n",
    "\n",
    "\n",
    "pattern = pattern.prune()\n",
    "\n",
    "pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Location For Output\n",
    "For this example, we are creating a temporary directory to write the data to. If we wanted to persist this Kerchunk reference, we could write to local disc or cloud storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "td = TemporaryDirectory()\n",
    "target_root = td.name\n",
    "target_root = \"esip\"\n",
    "store_name = \"PF_kerchunk\"\n",
    "target_store = os.path.join(target_root, store_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Pangeo-Forge Beam Pipeline\n",
    "\n",
    "Next, we will chain together a bunch of methods to create a Pangeo-Forge - Apache Beam pipeline. \n",
    "Processing steps are chained together with the pipe operator (`|`). Once the pipeline is built, it can be ran in the following cell. \n",
    "\n",
    "The steps are as follows:\n",
    "1. Creates a starting collection of our input file patterns.\n",
    "2. Passes those file_patterns to `OpenWithKerchunk`, which creates references of each file.\n",
    "3. Combines the references files into a single reference file with `CombineReferences`.\n",
    "4. Writes the combined reference file.\n",
    "\n",
    "Note: You can add additional processing steps in this pipeline. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from pangeo_forge_recipes.transforms import (\n",
    "    CombineReferences,\n",
    "    OpenWithKerchunk,\n",
    "    WriteCombinedReference,\n",
    ")\n",
    "\n",
    "transforms = (\n",
    "    # Create a beam PCollection from our input file pattern\n",
    "    beam.Create(pattern.items())\n",
    "    # Open with Kerchunk and create references for each file\n",
    "    | OpenWithKerchunk(file_type=pattern.file_type)\n",
    "    # Use Kerchunk's `MultiZarrToZarr` functionality to combine the reference files into a single\n",
    "    # reference file. *Note*: Setting the correct contact_dims and identical_dims is important.\n",
    "    | CombineReferences(\n",
    "        concat_dims=[\"day\"],\n",
    "        identical_dims=[\"lat\", \"lon\", \"crs\"],\n",
    "    )\n",
    "    # Write the combined Kerchunk reference to file.\n",
    "    | WriteCombinedReference(target_root=target_root, store_name=store_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    p | transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fsspec\n",
    "\n",
    "full_path = os.path.join(target_root, store_name, \"reference.json\")\n",
    "print(os.path.getsize(full_path) / 1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our reference .json file is about 1MB, instead of 108GBs. That is quite the storage savings! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(\n",
    "    \"reference://\",\n",
    "    fo=full_path,\n",
    "    remote_protocol=\"http\",\n",
    ")\n",
    "ds = xr.open_dataset(\n",
    "    mapper, engine=\"zarr\", decode_coords=\"all\", backend_kwargs={\"consolidated\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.isel(day=220).burning_index_g.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Access Speed Benchmark - Kerchunk vs NetCDF\n",
    "\n",
    "In the access test below, we had almost a 3x speedup in access time using the `Kerchunk` reference dataset vs the NetCDF file collection. This isn't a huge speed-up, but will vary a lot depending on chunking schema, access patterns etc. \n",
    "| Kerchunk      | Time (s)    |\n",
    "| ------------- | ----------- |\n",
    "| Kerchunk      | 10          |\n",
    "| Cloud NetCDF  | 28          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kerchunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kerchunk_path = os.path.join(target_root, store_name, \"reference.json\")\n",
    "\n",
    "mapper = fsspec.get_mapper(\n",
    "    \"reference://\",\n",
    "    fo=kerchunk_path,\n",
    "    remote_protocol=\"http\",\n",
    ")\n",
    "kerchunk_ds = xr.open_dataset(\n",
    "    mapper, engine=\"zarr\", decode_coords=\"all\", backend_kwargs={\"consolidated\": False}\n",
    ")\n",
    "kerchunk_ds.sel(lat=slice(48, 47), lon=slice(-123, -122)).burning_index_g.max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kerchunk_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took almost 10 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NetCDF Cloud Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare urls\n",
    "\n",
    "\n",
    "def url_gen(year):\n",
    "    return (\n",
    "        f\"http://thredds.northwestknowledge.net:8080/thredds/dodsC/MET/bi/bi_{year}.nc\"\n",
    "    )\n",
    "\n",
    "\n",
    "urls_list = [url_gen(year) for year in years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "netcdf_ds = xr.open_mfdataset(urls_list, engine=\"netcdf4\")\n",
    "netcdf_ds.sel(lat=slice(48, 47), lon=slice(-123, -122)).burning_index_g.mean().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "That took about about 28 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "netcdf_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Storage Benchmark - Kerchunk vs NetCDF \n",
    "## 5200x Storage Savings\n",
    "\n",
    "| Storage       | Mb (s)      |\n",
    "| ------------- | ----------- |\n",
    "| Kerchunk      | 10          |\n",
    "| Cloud NetCDF  | 52122       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kerchunk Reference File\n",
    "import os\n",
    "\n",
    "print(f\"{round(os.path.getsize(kerchunk_path) / 1e6, 1)} Mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NetCDF Files\n",
    "print(f\"{round(netcdf_ds.nbytes/1e6,1)} Mb\")\n",
    "print(\"or\")\n",
    "print(f\"{round(netcdf_ds.nbytes/1e9,1)} Gb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerchunk-cookbook-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
