{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../thumbnail.png\" width=500 alt=\"Kerchunk Logo\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of virtual Zarr stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is intended as an introduction to creating and using virtual Zarr stores.\n",
    "In this tutorial we will:\n",
    "- Scan a single NetCDF4/HDF5 file to create a virtual dataset\n",
    "- Learn how to use the output using `Xarray` and `Zarr`.\n",
    "\n",
    "While this notebook only examines using `VirtualiZarr` and `Kerchunk` on a single NetCDF file, these libraries can be used to create virtual `Zarr` datasets from collections of many input files. In the following notebook, we will demonstrate this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Helpful | Basic features |\n",
    "\n",
    "- **Time to learn**: 60 minutes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Here we will import a few `Python` libraries to help with our data processing. \n",
    "- `virtualizarr` will be used to generate the virtual Zarr store\n",
    "- `Xarray` for examining the output dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from virtualizarr import open_virtual_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define storage_options arguments\n",
    "\n",
    "In the dictionary definition in the next cell, we are defining the options that will be passed to [`fsspec.open`](https://filesystem-spec.readthedocs.io/en/latest/api.html?highlight=.open#fsspec.open). Any additional kwargs passed in this dictionary through `fsspec.open` will pass as kwargs to the file system, in our case `s3`. The API docs for the `s3fs` filesystem spec can be found [here](https://s3fs.readthedocs.io/en/latest/api.html).\n",
    "\n",
    "In this example we are passing a few kwargs. In short they are:\n",
    "- `anon=True`: This is a `s3fs` kwarg that specifies you are not passing any connection credentials and are connecting to a public bucket.\n",
    "- `default_fill_cache=False`: `s3fs` kwarg that avoids caching in between chunks of files. This may lower memory usage when reading large files.\n",
    "- `default_cache_type=\"none\"`: `fsspec` kwarg that specifies the caching strategy used by `fsspec`. In this case, we turn off caching entirely to lower memory usage when only using the information from the file once..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_options = dict(anon=True, default_fill_cache=False, default_cache_type=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtualize a single NetCDF file\n",
    "\n",
    "Below we will virtualize a NetCDF file stored on the AWS cloud. This dataset is a single time slice of a climate downscaled product for Alaska.\n",
    "\n",
    "The steps in the cell below are as follows:\n",
    "1. Create a virtual dataset using `open_virtual_dataset`\n",
    "1. Write the virtual store as a Kerchunk reference JSON using the `to_kerchunk` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input URL to dataset. Note this is a netcdf file stored on s3 (cloud dataset).\n",
    "url = \"s3://wrf-se-ak-ar5/ccsm/rcp85/daily/2060/WRFDS_2060-01-01.nc\"\n",
    "\n",
    "\n",
    "# Create a virtual dataset using VirtualiZarr.\n",
    "# We specify `indexes={}` to avoid creating in-memory pandas indexes for each 1D coordinate, since concatenating with pandas indexes is not yet supported in VirtualiZarr\n",
    "virtual_ds = open_virtual_dataset(\n",
    "    url, indexes={}, reader_options={\"storage_options\": storage_options}\n",
    ")\n",
    "# Write the virtual dataset to disk as a Kerchunk JSON. We could alternative write to a Kerchunk JSON or Icechunk Store.\n",
    "virtual_ds.virtualize.to_kerchunk(\"single_file_kerchunk.json\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening virtual datasets\n",
    "\n",
    "In the section below we will use the previously created `Kerchunk` reference JSON to open the `NetCDF` file as if it were a `Zarr` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We once again need to provide information for fsspec to access the remote file\n",
    "storage_options = dict(\n",
    "    remote_protocol=\"s3\", remote_options=dict(anon=True), skip_instance_cache=True\n",
    ")\n",
    "# We will use the \"kerchunk\" engine in `xr.open_dataset` and pass the `storage_options` to the `kerchunk` engine through `backend_kwargs`\n",
    "ds = xr.open_dataset(\n",
    "    \"single_file_kerchunk.json\",\n",
    "    engine=\"kerchunk\",\n",
    "    backend_kwargs={\"storage_options\": storage_options},\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.TMAX.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the original .nc file size here is 16.8MB, and the created JSON is 26.5kB. These files also tend to compress very well. As you can see, it the JSON can be written anywhere, and gives us access to the underlying data, reading only the chunks we need from remote without downloading the whole file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerchunk-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
